{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b80b80",
   "metadata": {
    "papermill": {
     "duration": 0.007929,
     "end_time": "2025-02-06T10:39:48.719218",
     "exception": false,
     "start_time": "2025-02-06T10:39:48.711289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be71d562",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6abd04f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:39:48.735099Z",
     "iopub.status.busy": "2025-02-06T10:39:48.734699Z",
     "iopub.status.idle": "2025-02-06T10:41:46.757102Z",
     "shell.execute_reply": "2025-02-06T10:41:46.755498Z"
    },
    "papermill": {
     "duration": 118.033036,
     "end_time": "2025-02-06T10:41:46.759563",
     "exception": false,
     "start_time": "2025-02-06T10:39:48.726527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.18.0\r\n",
      "  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.18.0) (1.4.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.18.0) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.18.0) (24.3.25)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.18.0) (0.6.0)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.18.0) (0.2.0)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.18.0) (18.1.1)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.18.0) (3.3.0)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.18.0) (24.1)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.18.0) (3.20.3)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.18.0) (2.32.3)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.18.0) (71.0.4)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.18.0) (1.16.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.18.0) (2.4.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.18.0) (4.12.2)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.18.0) (1.16.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.18.0) (1.64.1)\r\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow==2.18.0)\r\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Collecting keras>=3.5.0 (from tensorflow==2.18.0)\r\n",
      "  Downloading keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\r\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.18.0) (1.26.4)\r\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.18.0) (3.11.0)\r\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.18.0) (0.4.1)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.18.0) (0.37.1)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.18.0) (0.44.0)\r\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow==2.18.0) (13.8.1)\r\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow==2.18.0) (0.0.8)\r\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow==2.18.0) (0.12.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2024.8.30)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.7)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.0.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow==2.18.0) (2.1.5)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow==2.18.0) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow==2.18.0) (2.18.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.18.0) (0.1.2)\r\n",
      "Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading keras-3.8.0-py3-none-any.whl (1.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: tensorboard, keras, tensorflow\r\n",
      "  Attempting uninstall: tensorboard\r\n",
      "    Found existing installation: tensorboard 2.17.0\r\n",
      "    Uninstalling tensorboard-2.17.0:\r\n",
      "      Successfully uninstalled tensorboard-2.17.0\r\n",
      "  Attempting uninstall: keras\r\n",
      "    Found existing installation: keras 3.4.1\r\n",
      "    Uninstalling keras-3.4.1:\r\n",
      "      Successfully uninstalled keras-3.4.1\r\n",
      "  Attempting uninstall: tensorflow\r\n",
      "    Found existing installation: tensorflow 2.17.0\r\n",
      "    Uninstalling tensorflow-2.17.0:\r\n",
      "      Successfully uninstalled tensorflow-2.17.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.18.0 which is incompatible.\r\n",
      "tensorflow-text 2.17.0 requires tensorflow<2.18,>=2.17.0, but you have tensorflow 2.18.0 which is incompatible.\r\n",
      "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed keras-3.8.0 tensorboard-2.18.0 tensorflow-2.18.0\r\n",
      "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (1.26.4)\r\n",
      "Collecting tensorflow_probability==0.25.0\r\n",
      "  Downloading tensorflow_probability-0.25.0-py2.py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow_probability==0.25.0) (1.4.0)\r\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_probability==0.25.0) (1.16.0)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow_probability==0.25.0) (1.26.4)\r\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow_probability==0.25.0) (4.4.2)\r\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow_probability==0.25.0) (3.1.0)\r\n",
      "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow_probability==0.25.0) (0.6.0)\r\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow_probability==0.25.0) (0.1.8)\r\n",
      "Downloading tensorflow_probability-0.25.0-py2.py3-none-any.whl (7.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: tensorflow_probability\r\n",
      "  Attempting uninstall: tensorflow_probability\r\n",
      "    Found existing installation: tensorflow-probability 0.24.0\r\n",
      "    Uninstalling tensorflow-probability-0.24.0:\r\n",
      "      Successfully uninstalled tensorflow-probability-0.24.0\r\n",
      "Successfully installed tensorflow_probability-0.25.0\r\n",
      "Collecting keras==3.7.0\r\n",
      "  Downloading keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\r\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras==3.7.0) (1.4.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras==3.7.0) (1.26.4)\r\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras==3.7.0) (13.8.1)\r\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras==3.7.0) (0.0.8)\r\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras==3.7.0) (3.11.0)\r\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras==3.7.0) (0.12.1)\r\n",
      "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras==3.7.0) (0.4.1)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras==3.7.0) (24.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras==3.7.0) (4.12.2)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras==3.7.0) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras==3.7.0) (2.18.0)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras==3.7.0) (0.1.2)\r\n",
      "Downloading keras-3.7.0-py3-none-any.whl (1.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: keras\r\n",
      "  Attempting uninstall: keras\r\n",
      "    Found existing installation: keras 3.8.0\r\n",
      "    Uninstalling keras-3.8.0:\r\n",
      "      Successfully uninstalled keras-3.8.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.18.0 which is incompatible.\r\n",
      "tensorflow-text 2.17.0 requires tensorflow<2.18,>=2.17.0, but you have tensorflow 2.18.0 which is incompatible.\r\n",
      "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed keras-3.7.0\r\n",
      "Collecting pyDOE\r\n",
      "  Downloading pyDOE-0.3.8.zip (22 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyDOE) (1.26.4)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyDOE) (1.13.1)\r\n",
      "Building wheels for collected packages: pyDOE\r\n",
      "  Building wheel for pyDOE (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for pyDOE: filename=pyDOE-0.3.8-py3-none-any.whl size=18171 sha256=4be92c37200ae89f16de1bdaa7115cb6cd8f5ab2792ed732db3cb22bd802d273\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/ce/b6/d7/c6b64746dba6433c593e471e0ac3acf4f36040456d1d160d17\r\n",
      "Successfully built pyDOE\r\n",
      "Installing collected packages: pyDOE\r\n",
      "Successfully installed pyDOE-0.3.8\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow==2.18.0\n",
    "!pip install --upgrade numpy==1.26.4\n",
    "!pip install  --upgrade tensorflow_probability==0.25.0\n",
    "!pip install --upgrade keras==3.7.0\n",
    "# !pip install pyDOE\n",
    "!pip install pyDOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa84bb",
   "metadata": {
    "papermill": {
     "duration": 0.019748,
     "end_time": "2025-02-06T10:41:46.799561",
     "exception": false,
     "start_time": "2025-02-06T10:41:46.779813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f493c8e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:41:46.838620Z",
     "iopub.status.busy": "2025-02-06T10:41:46.838222Z",
     "iopub.status.idle": "2025-02-06T10:41:46.843300Z",
     "shell.execute_reply": "2025-02-06T10:41:46.842043Z"
    },
    "papermill": {
     "duration": 0.02681,
     "end_time": "2025-02-06T10:41:46.844976",
     "exception": false,
     "start_time": "2025-02-06T10:41:46.818166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    " #!pip install  --upgrade tensorflow_probability==0.25.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af80e16a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:41:46.885769Z",
     "iopub.status.busy": "2025-02-06T10:41:46.885361Z",
     "iopub.status.idle": "2025-02-06T10:41:54.115261Z",
     "shell.execute_reply": "2025-02-06T10:41:54.114260Z"
    },
    "papermill": {
     "duration": 7.253142,
     "end_time": "2025-02-06T10:41:54.117319",
     "exception": false,
     "start_time": "2025-02-06T10:41:46.864177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../../Utilities/')\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "from pyDOE import lhs\n",
    "'from plotting import newfig, savefig'\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import time\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from keras import models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c483e125",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:41:54.157496Z",
     "iopub.status.busy": "2025-02-06T10:41:54.156802Z",
     "iopub.status.idle": "2025-02-06T10:41:54.170584Z",
     "shell.execute_reply": "2025-02-06T10:41:54.169651Z"
    },
    "papermill": {
     "duration": 0.036208,
     "end_time": "2025-02-06T10:41:54.172596",
     "exception": false,
     "start_time": "2025-02-06T10:41:54.136388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "zzz =tf.keras.models\n",
    "zz = tf.keras.models.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1745d023",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:41:54.212438Z",
     "iopub.status.busy": "2025-02-06T10:41:54.212064Z",
     "iopub.status.idle": "2025-02-06T10:41:54.215955Z",
     "shell.execute_reply": "2025-02-06T10:41:54.214983Z"
    },
    "papermill": {
     "duration": 0.025097,
     "end_time": "2025-02-06T10:41:54.217599",
     "exception": false,
     "start_time": "2025-02-06T10:41:54.192502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#kernel_initializer='glorot_normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a50b90e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:41:54.256720Z",
     "iopub.status.busy": "2025-02-06T10:41:54.256354Z",
     "iopub.status.idle": "2025-02-06T10:41:54.395419Z",
     "shell.execute_reply": "2025-02-06T10:41:54.394080Z"
    },
    "papermill": {
     "duration": 0.160947,
     "end_time": "2025-02-06T10:41:54.397283",
     "exception": false,
     "start_time": "2025-02-06T10:41:54.236336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ad6027",
   "metadata": {
    "papermill": {
     "duration": 0.018816,
     "end_time": "2025-02-06T10:41:54.435626",
     "exception": false,
     "start_time": "2025-02-06T10:41:54.416810",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42f4c083",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:41:54.477050Z",
     "iopub.status.busy": "2025-02-06T10:41:54.476633Z",
     "iopub.status.idle": "2025-02-06T10:41:54.610838Z",
     "shell.execute_reply": "2025-02-06T10:41:54.609464Z"
    },
    "papermill": {
     "duration": 0.158369,
     "end_time": "2025-02-06T10:41:54.613505",
     "exception": false,
     "start_time": "2025-02-06T10:41:54.455136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: conda: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!conda install python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d5eb0ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:41:54.655380Z",
     "iopub.status.busy": "2025-02-06T10:41:54.654944Z",
     "iopub.status.idle": "2025-02-06T10:41:54.790454Z",
     "shell.execute_reply": "2025-02-06T10:41:54.789239Z"
    },
    "papermill": {
     "duration": 0.157716,
     "end_time": "2025-02-06T10:41:54.792445",
     "exception": false,
     "start_time": "2025-02-06T10:41:54.634729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34177b12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:41:54.831684Z",
     "iopub.status.busy": "2025-02-06T10:41:54.831343Z",
     "iopub.status.idle": "2025-02-06T10:41:54.970299Z",
     "shell.execute_reply": "2025-02-06T10:41:54.969042Z"
    },
    "papermill": {
     "duration": 0.160963,
     "end_time": "2025-02-06T10:41:54.972427",
     "exception": false,
     "start_time": "2025-02-06T10:41:54.811464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: missing file operand\r\n",
      "Try 'ln --help' for more information.\r\n"
     ]
    }
   ],
   "source": [
    "!ln \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bf0b8ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:41:55.011896Z",
     "iopub.status.busy": "2025-02-06T10:41:55.011521Z",
     "iopub.status.idle": "2025-02-06T10:41:55.091184Z",
     "shell.execute_reply": "2025-02-06T10:41:55.090066Z"
    },
    "papermill": {
     "duration": 0.101313,
     "end_time": "2025-02-06T10:41:55.093035",
     "exception": false,
     "start_time": "2025-02-06T10:41:54.991722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "noise = 0.0        \n",
    "layers = [2, 100, 100, 100, 100, 2]\n",
    "    \n",
    "\n",
    "# Doman bounds\n",
    "lb = np.array([-5.0, 0.0])\n",
    "ub = np.array([5.0, np.pi/2])\n",
    "\n",
    "N0 = 50\n",
    "N_b = 50\n",
    "N_f = 20000\n",
    "\n",
    "data = scipy.io.loadmat('/kaggle/input/shroed/NLS.mat')\n",
    "\n",
    "t = data['tt'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = data['uu']\n",
    "Exact_u = np.real(Exact)\n",
    "Exact_v = np.imag(Exact)\n",
    "Exact_h = np.sqrt(Exact_u**2 + Exact_v**2)\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact_u.T.flatten()[:,None]\n",
    "v_star = Exact_v.T.flatten()[:,None]\n",
    "h_star = Exact_h.T.flatten()[:,None]\n",
    "\n",
    "###########################\n",
    "np.random.seed(1234)\n",
    "\n",
    "idx_x = np.random.choice(x.shape[0], N0, replace=False)\n",
    "x0 = x[idx_x,:]\n",
    "u0 = Exact_u[idx_x,0:1]\n",
    "v0 = Exact_v[idx_x,0:1]\n",
    "\n",
    "idx_t = np.random.choice(t.shape[0], N_b, replace=False)\n",
    "tb = t[idx_t,:]\n",
    "\n",
    "X_f = lb + (ub-lb)*lhs(2, N_f)\n",
    "#X0 = np.concatenate((x0, 0*x0), 1) # (x0, 0)\n",
    "#X_lb = np.concatenate((0*tb + lb[0], tb), 1) # (lb[0], tb)\n",
    "#X_ub = np.concatenate((0*tb + ub[0], tb), 1) # (ub[0], tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93296e33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:41:55.133121Z",
     "iopub.status.busy": "2025-02-06T10:41:55.132701Z",
     "iopub.status.idle": "2025-02-06T10:41:55.166793Z",
     "shell.execute_reply": "2025-02-06T10:41:55.165702Z"
    },
    "papermill": {
     "duration": 0.056002,
     "end_time": "2025-02-06T10:41:55.168736",
     "exception": false,
     "start_time": "2025-02-06T10:41:55.112734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parameters( x0, u0, v0, tb, X_f, lb, ub):\n",
    "       \n",
    "       X0 = np.concatenate((x0, 0*x0), 1) # (x0, 0)\n",
    "       X_lb = np.concatenate((0*tb + lb[0], tb), 1) # (lb[0], tb)\n",
    "       X_ub = np.concatenate((0*tb + ub[0], tb), 1) # (ub[0], tb)\n",
    "       \n",
    "              \n",
    "       x0 = X0[:,0:1]\n",
    "       t0 = X0[:,1:2]\n",
    "\n",
    "       x_lb = X_lb[:,0:1]\n",
    "       t_lb = X_lb[:,1:2]\n",
    "\n",
    "       x_ub = X_ub[:,0:1]\n",
    "       t_ub = X_ub[:,1:2]\n",
    "       \n",
    "       x_f = X_f[:,0:1]\n",
    "       t_f = X_f[:,1:2]\n",
    "       \n",
    "       u0 = u0\n",
    "       v0 = v0\n",
    "       \n",
    "       q = tf.constant(x0,tf.float32)\n",
    "       t0_tf = tf.constant(t0,tf.float32)\n",
    "\n",
    "       u0_tf = tf.constant(u0,tf.float32)\n",
    "       v0_tf = tf.constant(v0,tf.float32)\n",
    "\n",
    "       x_lb_tf = tf.constant(x_lb,tf.float32)\n",
    "       t_lb_tf = tf.constant(t_lb,tf.float32)\n",
    "\n",
    "       x_ub_tf = tf.constant(x_ub,tf.float32)\n",
    "       t_ub_tf = tf.constant(t_ub,tf.float32)\n",
    "\n",
    "       x_f_tf = tf.constant(x_f,tf.float32)\n",
    "       t_f_tf = tf.constant(t_f,tf.float32)\n",
    "       return(q,t0_tf,u0_tf,v0_tf,x_lb_tf,t_lb_tf,x_ub_tf,t_ub_tf,x_f_tf,t_f_tf)\n",
    "       # Initialize NNs\n",
    "q,t0_tf,u0_tf,v0_tf,x_lb_tf,t_lb_tf,x_ub_tf,t_ub_tf,x_f_tf,t_f_tf = parameters(x0, u0, v0, tb, X_f, lb, ub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d11dbc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:41:55.208302Z",
     "iopub.status.busy": "2025-02-06T10:41:55.207951Z",
     "iopub.status.idle": "2025-02-06T10:41:55.271077Z",
     "shell.execute_reply": "2025-02-06T10:41:55.269898Z"
    },
    "papermill": {
     "duration": 0.085246,
     "end_time": "2025-02-06T10:41:55.273051",
     "exception": false,
     "start_time": "2025-02-06T10:41:55.187805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#kein neues netz\n",
    "def initialize_NN( ebenen):        \n",
    "       components = []\n",
    "       biases = []\n",
    "       c0 = tf.keras.Input(shape=(ebenen[0],))\n",
    "       components.append(c0)  \n",
    "       num_ebenen = len(ebenen) \n",
    "       for l in range(1,num_ebenen):\n",
    "              c =  tf.keras.layers.Dense( (ebenen[l]), activation='tanh')\n",
    "              components.append(c)       \n",
    "       return components\n",
    "def NN_manual():        \n",
    "       \n",
    "       biases = []\n",
    "       z1 = zzz.Sequential([\n",
    "        tf.keras.Input(shape=(2,)),\n",
    "        tf.keras.layers.Dense(100, activation='tanh'),\n",
    "        tf.keras.layers.Dense(100, activation='tanh'),\n",
    "        tf.keras.layers.Dense(100, activation='tanh'),\n",
    "        tf.keras.layers.Dense(100, activation='tanh'),\n",
    "        tf.keras.layers.Dense( 2)\n",
    "       ])\n",
    "       return z1\n",
    "\n",
    "def neural_net(components):\n",
    "       \n",
    "       I = components[0]\n",
    "       num_ebenen =  len(components)\n",
    "       for l in range(1,num_ebenen -1):\n",
    "              I = components[l](I)\n",
    "       \n",
    "       output_tensor = components[(num_ebenen-1)](I)\n",
    "       return zz(components[0],output_tensor)\n",
    "components = initialize_NN(layers)\n",
    "nn = neural_net(components)# außerhalb?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80b05cd2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:41:55.312572Z",
     "iopub.status.busy": "2025-02-06T10:41:55.312189Z",
     "iopub.status.idle": "2025-02-06T10:41:55.356181Z",
     "shell.execute_reply": "2025-02-06T10:41:55.355250Z"
    },
    "papermill": {
     "duration": 0.066779,
     "end_time": "2025-02-06T10:41:55.358956",
     "exception": false,
     "start_time": "2025-02-06T10:41:55.292177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "l0 = tf.keras.layers.Input(shape=(2,))\n",
    "l = tf.keras.layers.Dense(100, activation='tanh')(l0)\n",
    "l = tf.keras.layers.Dense(100, activation='tanh')(l)\n",
    "l = tf.keras.layers.Dense(100, activation='tanh')(l)\n",
    "l = tf.keras.layers.Dense(100, activation='tanh')(l)\n",
    "l = tf.keras.layers.Dense( 2)(l) #error\n",
    "t = zz(l0,l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86202bae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:41:55.399830Z",
     "iopub.status.busy": "2025-02-06T10:41:55.399436Z",
     "iopub.status.idle": "2025-02-06T10:41:55.404291Z",
     "shell.execute_reply": "2025-02-06T10:41:55.403234Z"
    },
    "papermill": {
     "duration": 0.026751,
     "end_time": "2025-02-06T10:41:55.406055",
     "exception": false,
     "start_time": "2025-02-06T10:41:55.379304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def net(x,t):\n",
    "    uv =  nn(tf.concat([x,t],1))\n",
    "    u = uv[:,0:1]\n",
    "    v = uv[:,1:2]\n",
    "    return u,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1290eb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:41:55.446017Z",
     "iopub.status.busy": "2025-02-06T10:41:55.445601Z",
     "iopub.status.idle": "2025-02-06T10:41:55.453307Z",
     "shell.execute_reply": "2025-02-06T10:41:55.452169Z"
    },
    "papermill": {
     "duration": 0.029692,
     "end_time": "2025-02-06T10:41:55.455138",
     "exception": false,
     "start_time": "2025-02-06T10:41:55.425446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def net_uv( x, t):\n",
    "       with tf.GradientTape(persistent=True) as tape:\n",
    "              tape.watch(x)\n",
    "              tape.watch(t)\n",
    "              uv = nn( tf.concat([x,t],1))\n",
    "              u = uv[:,0:1]\n",
    "              v = uv[:,1:2]\n",
    "              # tf.print(tape.gradient(u,x))\n",
    "              u_x = tape.gradient(u, x)[0]\n",
    "              v_x = tape.gradient(v, x)[0]\n",
    "       return u, v, u_x, v_x\n",
    "def net_f_uv(x, t):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "       tape.watch(x)\n",
    "       tape.watch(t)\n",
    "       u, v, u_x, v_x = net_uv(x,t)\n",
    "       \n",
    "       u_t = tape.gradient(u, t)[0]\n",
    "       u_xx = tape.gradient(u_x, x)[0]\n",
    "       \n",
    "       v_t = tape.gradient(v, t)[0]\n",
    "       v_xx = tape.gradient(v_x, x)[0]\n",
    "        \n",
    "    f_u = u_t + 0.5*v_xx + (u**2 + v**2)*v\n",
    "    f_v = v_t - 0.5*u_xx - (u**2 + v**2)*u   \n",
    "    \n",
    "    return f_u, f_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bf258b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:41:55.495270Z",
     "iopub.status.busy": "2025-02-06T10:41:55.494885Z",
     "iopub.status.idle": "2025-02-06T10:41:55.500671Z",
     "shell.execute_reply": "2025-02-06T10:41:55.499669Z"
    },
    "papermill": {
     "duration": 0.027388,
     "end_time": "2025-02-06T10:41:55.502331",
     "exception": false,
     "start_time": "2025-02-06T10:41:55.474943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Loss\n",
    "def Loss2(u0_pred, v0_pred,u_lb_pred, v_lb_pred, u_x_lb_pred, v_x_lb_pred,u_ub_pred, v_ub_pred, u_x_ub_pred, v_x_ub_pred,f_u_pred, f_v_pred):\n",
    "       loss = tf.reduce_mean(input_tensor=tf.square(u0_tf - u0_pred)) + \\\n",
    "              tf.reduce_mean(input_tensor=tf.square(v0_tf - v0_pred)) + \\\n",
    "              tf.reduce_mean(input_tensor=tf.square(u_lb_pred - u_ub_pred)) + \\\n",
    "              tf.reduce_mean(input_tensor=tf.square(v_lb_pred - v_ub_pred)) + \\\n",
    "              tf.reduce_mean(input_tensor=tf.square(u_x_lb_pred - u_x_ub_pred)) + \\\n",
    "              tf.reduce_mean(input_tensor=tf.square(v_x_lb_pred - v_x_ub_pred)) + \\\n",
    "              tf.reduce_mean(input_tensor=tf.square(f_u_pred)) + \\\n",
    "              tf.reduce_mean(input_tensor=tf.square(f_v_pred))\n",
    "       return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fa1b99c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:41:55.542384Z",
     "iopub.status.busy": "2025-02-06T10:41:55.541996Z",
     "iopub.status.idle": "2025-02-06T10:41:55.547127Z",
     "shell.execute_reply": "2025-02-06T10:41:55.545936Z"
    },
    "papermill": {
     "duration": 0.02693,
     "end_time": "2025-02-06T10:41:55.548590",
     "exception": false,
     "start_time": "2025-02-06T10:41:55.521660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def callback(self, xr=None):\n",
    "    if self.iter % 50 == 0:\n",
    "        print('It {:05d}: loss = {:10.8e}'.format(self.iter,self.current_loss))\n",
    "    self.hist.append(self.current_loss)\n",
    "    self.iter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06513e0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:41:55.588165Z",
     "iopub.status.busy": "2025-02-06T10:41:55.587706Z",
     "iopub.status.idle": "2025-02-06T10:41:55.676218Z",
     "shell.execute_reply": "2025-02-06T10:41:55.675057Z"
    },
    "papermill": {
     "duration": 0.110562,
     "end_time": "2025-02-06T10:41:55.678436",
     "exception": false,
     "start_time": "2025-02-06T10:41:55.567874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn = NN_manual()\n",
    "nn.load_weights(\"/kaggle/input/sec4/keras/default/1/nn2_3.weights.h5\")\n",
    "train_loss_record2 = []\n",
    "\n",
    "#lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay([1000,3000],[1e-2,1e-3,5e-4])\n",
    "lr = 5e-4\n",
    "# Choose the optimizer\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f8afee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-11T10:36:37.602799Z",
     "iopub.status.busy": "2025-01-11T10:36:37.602394Z",
     "iopub.status.idle": "2025-01-11T10:36:37.607599Z",
     "shell.execute_reply": "2025-01-11T10:36:37.606238Z",
     "shell.execute_reply.started": "2025-01-11T10:36:37.602763Z"
    },
    "papermill": {
     "duration": 0.018683,
     "end_time": "2025-02-06T10:41:55.716466",
     "exception": false,
     "start_time": "2025-02-06T10:41:55.697783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "899aa9d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:41:55.758304Z",
     "iopub.status.busy": "2025-02-06T10:41:55.757942Z",
     "iopub.status.idle": "2025-02-06T10:41:58.724971Z",
     "shell.execute_reply": "2025-02-06T10:41:58.723690Z"
    },
    "papermill": {
     "duration": 2.989727,
     "end_time": "2025-02-06T10:41:58.727153",
     "exception": false,
     "start_time": "2025-02-06T10:41:55.737426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = 5e-4\n",
    "train_loss_record2 = []\n",
    "train_vars = nn.trainable_variables\n",
    "# Choose the optimizer\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "train_loss_record2 = []\n",
    "for i in range(2):\n",
    "    with tf.GradientTape() as t:                 \n",
    "            u0_pred, v0_pred = net(q, t0_tf)\n",
    "            u_lb_pred, v_lb_pred, u_x_lb_pred, v_x_lb_pred = net_uv(x_lb_tf, t_lb_tf)\n",
    "            u_ub_pred, v_ub_pred, u_x_ub_pred, v_x_ub_pred = net_uv(x_ub_tf, t_ub_tf)\n",
    "            f_u_pred, f_v_pred = net_f_uv(x_f_tf, t_f_tf)\n",
    "            loss = Loss2(u0_pred, v0_pred,u_lb_pred, v_lb_pred, u_x_lb_pred, v_x_lb_pred,u_ub_pred, v_ub_pred, u_x_ub_pred, v_x_ub_pred,f_u_pred, f_v_pred)\n",
    "            grad = t.gradient(loss, train_vars)\n",
    "            optim.apply_gradients(zip(grad, train_vars))\n",
    "            train_loss_record2.append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b475ec34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:41:58.767037Z",
     "iopub.status.busy": "2025-02-06T10:41:58.766628Z",
     "iopub.status.idle": "2025-02-06T10:41:58.938043Z",
     "shell.execute_reply": "2025-02-06T10:41:58.936778Z"
    },
    "papermill": {
     "duration": 0.193664,
     "end_time": "2025-02-06T10:41:58.940064",
     "exception": false,
     "start_time": "2025-02-06T10:41:58.746400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "z2 = {}\n",
    "for a in range(optim.variables.__len__()):\n",
    "   \n",
    "    k = str(i) \n",
    "    a = np.loadtxt('/kaggle/input/adam3v/var'+str(i)+'.csv')\n",
    "    z2[k] = a\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "054aa47a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:41:58.980773Z",
     "iopub.status.busy": "2025-02-06T10:41:58.980432Z",
     "iopub.status.idle": "2025-02-06T10:41:58.994133Z",
     "shell.execute_reply": "2025-02-06T10:41:58.993179Z"
    },
    "papermill": {
     "duration": 0.035611,
     "end_time": "2025-02-06T10:41:58.996255",
     "exception": false,
     "start_time": "2025-02-06T10:41:58.960644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "optim.load_own_variables(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "571d5cb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:41:59.035891Z",
     "iopub.status.busy": "2025-02-06T10:41:59.035508Z",
     "iopub.status.idle": "2025-02-06T10:41:59.043943Z",
     "shell.execute_reply": "2025-02-06T10:41:59.042699Z"
    },
    "papermill": {
     "duration": 0.030449,
     "end_time": "2025-02-06T10:41:59.045907",
     "exception": false,
     "start_time": "2025-02-06T10:41:59.015458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "z = {}\n",
    "optim.save_own_variables(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d57d2959",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:41:59.085983Z",
     "iopub.status.busy": "2025-02-06T10:41:59.085592Z",
     "iopub.status.idle": "2025-02-06T10:41:59.092867Z",
     "shell.execute_reply": "2025-02-06T10:41:59.091879Z"
    },
    "papermill": {
     "duration": 0.029362,
     "end_time": "2025-02-06T10:41:59.094636",
     "exception": false,
     "start_time": "2025-02-06T10:41:59.065274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0741ff67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:41:59.134632Z",
     "iopub.status.busy": "2025-02-06T10:41:59.134283Z",
     "iopub.status.idle": "2025-02-06T10:41:59.140076Z",
     "shell.execute_reply": "2025-02-06T10:41:59.138990Z"
    },
    "papermill": {
     "duration": 0.027784,
     "end_time": "2025-02-06T10:41:59.141803",
     "exception": false,
     "start_time": "2025-02-06T10:41:59.114019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "faf817c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T10:41:59.183388Z",
     "iopub.status.busy": "2025-02-06T10:41:59.183044Z",
     "iopub.status.idle": "2025-02-06T17:00:09.099460Z",
     "shell.execute_reply": "2025-02-06T17:00:09.097214Z"
    },
    "papermill": {
     "duration": 22689.940601,
     "end_time": "2025-02-06T17:00:09.102496",
     "exception": false,
     "start_time": "2025-02-06T10:41:59.161895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 00000: loss = 9.93121299e-04\n",
      "It 00050: loss = 2.89566116e-04\n",
      "It 00100: loss = 8.25802199e-05\n",
      "It 00150: loss = 5.93334516e-05\n",
      "It 00200: loss = 4.72548491e-05\n",
      "It 00250: loss = 3.88319495e-05\n",
      "It 00300: loss = 3.25262918e-05\n",
      "It 00350: loss = 2.76585670e-05\n",
      "It 00400: loss = 3.07694027e-05\n",
      "It 00450: loss = 2.09415812e-05\n",
      "It 00500: loss = 1.84155651e-05\n",
      "It 00550: loss = 1.64793018e-05\n",
      "It 00600: loss = 3.49184047e-05\n",
      "It 00650: loss = 1.43630732e-05\n",
      "It 00700: loss = 1.27653739e-05\n",
      "It 00750: loss = 1.18617836e-05\n",
      "It 00800: loss = 1.14888480e-05\n",
      "It 00850: loss = 1.34959600e-05\n",
      "It 00900: loss = 1.03088132e-05\n",
      "It 00950: loss = 9.75515104e-06\n",
      "It 01000: loss = 1.88894264e-05\n",
      "It 01050: loss = 8.93079596e-06\n",
      "It 01100: loss = 9.63967614e-06\n",
      "It 01150: loss = 8.53759138e-06\n",
      "It 01200: loss = 8.05432865e-06\n",
      "It 01250: loss = 1.50375694e-04\n",
      "It 01300: loss = 8.55130020e-06\n",
      "It 01350: loss = 7.35603862e-06\n",
      "It 01400: loss = 7.09267897e-06\n",
      "It 01450: loss = 9.92256901e-06\n",
      "It 01500: loss = 6.99730754e-06\n",
      "It 01550: loss = 6.63134597e-06\n",
      "It 01600: loss = 3.97851727e-05\n",
      "It 01650: loss = 7.07406389e-06\n",
      "It 01700: loss = 6.65928746e-06\n",
      "It 01750: loss = 5.98667930e-06\n",
      "It 01800: loss = 5.66744029e-06\n",
      "It 01850: loss = 2.70230463e-04\n",
      "It 01900: loss = 6.51344772e-06\n",
      "It 01950: loss = 5.54666713e-06\n",
      "It 02000: loss = 5.25070254e-06\n",
      "It 02050: loss = 5.03686942e-06\n",
      "It 02100: loss = 1.50125306e-05\n",
      "It 02150: loss = 5.26618669e-06\n",
      "It 02200: loss = 4.89612648e-06\n",
      "It 02250: loss = 4.87092348e-06\n",
      "It 02300: loss = 7.50169556e-06\n",
      "It 02350: loss = 4.70158284e-06\n",
      "It 02400: loss = 1.84545097e-05\n",
      "It 02450: loss = 4.96136727e-06\n",
      "It 02500: loss = 4.55309964e-06\n",
      "It 02550: loss = 2.02408919e-05\n",
      "It 02600: loss = 2.51311485e-05\n",
      "It 02650: loss = 4.49449726e-06\n",
      "It 02700: loss = 4.24817745e-06\n",
      "It 02750: loss = 1.73757289e-05\n",
      "It 02800: loss = 2.20597994e-05\n",
      "It 02850: loss = 4.21827644e-06\n",
      "It 02900: loss = 4.71641306e-06\n",
      "It 02950: loss = 1.44019896e-05\n",
      "It 03000: loss = 2.60304132e-05\n",
      "It 03050: loss = 4.21108416e-06\n",
      "It 03100: loss = 5.23407562e-06\n",
      "It 03150: loss = 7.22552022e-06\n",
      "It 03200: loss = 4.19917433e-06\n",
      "It 03250: loss = 4.55791269e-06\n",
      "It 03300: loss = 1.87668811e-05\n",
      "It 03350: loss = 4.11986775e-06\n",
      "It 03400: loss = 5.93157747e-05\n",
      "It 03450: loss = 4.45423211e-06\n",
      "It 03500: loss = 3.99330966e-05\n",
      "It 03550: loss = 6.10081988e-05\n",
      "It 03600: loss = 4.13116959e-06\n",
      "It 03650: loss = 4.81284133e-06\n",
      "It 03700: loss = 3.95840289e-06\n",
      "It 03750: loss = 8.82195945e-06\n",
      "It 03800: loss = 3.96286487e-06\n",
      "It 03850: loss = 1.52898356e-05\n",
      "It 03900: loss = 4.04086768e-06\n",
      "It 03950: loss = 7.43917699e-05\n",
      "It 04000: loss = 4.27600753e-06\n",
      "It 04050: loss = 8.61853914e-05\n",
      "It 04100: loss = 4.30367891e-06\n",
      "It 04150: loss = 9.65296385e-06\n",
      "It 04200: loss = 6.62788716e-06\n",
      "It 04250: loss = 3.30700532e-05\n",
      "It 04300: loss = 4.44867510e-06\n",
      "It 04350: loss = 6.23044889e-06\n",
      "It 04400: loss = 4.47808270e-06\n",
      "It 04450: loss = 1.18877200e-04\n",
      "It 04500: loss = 4.40862959e-06\n",
      "It 04550: loss = 7.50022737e-05\n",
      "It 04600: loss = 4.16516423e-06\n",
      "It 04650: loss = 1.14585600e-05\n",
      "It 04700: loss = 5.86671877e-06\n",
      "It 04750: loss = 1.35820374e-05\n",
      "It 04800: loss = 4.29549618e-06\n",
      "It 04850: loss = 1.06195739e-05\n",
      "It 04900: loss = 3.47736604e-05\n",
      "It 04950: loss = 4.91755054e-06\n",
      "It 05000: loss = 8.85790905e-06\n",
      "It 05050: loss = 3.94087101e-06\n",
      "It 05100: loss = 2.23029474e-05\n",
      "It 05150: loss = 3.36978155e-05\n",
      "It 05200: loss = 4.51929827e-06\n",
      "It 05250: loss = 3.12848824e-05\n",
      "It 05300: loss = 3.79840412e-06\n",
      "It 05350: loss = 1.56951664e-05\n",
      "It 05400: loss = 3.80478014e-06\n",
      "It 05450: loss = 2.59244516e-05\n",
      "It 05500: loss = 3.92759239e-06\n",
      "It 05550: loss = 1.08097838e-05\n",
      "It 05600: loss = 5.28962210e-06\n",
      "It 05650: loss = 4.69919951e-06\n",
      "It 05700: loss = 4.54534529e-06\n",
      "It 05750: loss = 7.12333349e-06\n",
      "It 05800: loss = 2.28545814e-05\n",
      "It 05850: loss = 3.99208011e-06\n",
      "It 05900: loss = 7.95939013e-06\n",
      "It 05950: loss = 2.21681512e-05\n",
      "It 06000: loss = 3.86056900e-06\n",
      "It 06050: loss = 1.65844613e-05\n",
      "It 06100: loss = 3.95600046e-06\n",
      "It 06150: loss = 7.73809734e-06\n",
      "It 06200: loss = 1.42571716e-05\n",
      "It 06250: loss = 3.93849359e-06\n",
      "It 06300: loss = 7.99810914e-06\n",
      "It 06350: loss = 1.31818388e-05\n",
      "It 06400: loss = 4.16592457e-06\n",
      "It 06450: loss = 3.87298096e-05\n",
      "It 06500: loss = 4.83512622e-06\n",
      "It 06550: loss = 1.02894837e-05\n",
      "It 06600: loss = 5.81367385e-06\n",
      "It 06650: loss = 7.24501224e-05\n",
      "It 06700: loss = 3.54362419e-06\n",
      "It 06750: loss = 8.57965733e-06\n",
      "It 06800: loss = 3.55784323e-06\n",
      "It 06850: loss = 7.60786315e-06\n",
      "It 06900: loss = 3.55213274e-06\n",
      "It 06950: loss = 1.14172963e-05\n",
      "It 07000: loss = 3.86413012e-06\n",
      "It 07050: loss = 6.30482464e-06\n",
      "It 07100: loss = 5.28286955e-06\n",
      "It 07150: loss = 6.19531647e-06\n",
      "It 07200: loss = 4.73298178e-06\n",
      "It 07250: loss = 1.80185143e-05\n",
      "It 07300: loss = 1.96417277e-05\n",
      "It 07350: loss = 4.59923240e-06\n",
      "It 07400: loss = 6.91526802e-05\n",
      "It 07450: loss = 4.24309656e-06\n",
      "It 07500: loss = 4.71485946e-05\n",
      "It 07550: loss = 4.21812729e-06\n",
      "It 07600: loss = 2.27202454e-05\n",
      "It 07650: loss = 5.47432410e-06\n",
      "It 07700: loss = 3.55551583e-06\n",
      "It 07750: loss = 2.92305285e-05\n",
      "It 07800: loss = 3.56349096e-06\n",
      "It 07850: loss = 7.21327706e-06\n",
      "It 07900: loss = 3.86083775e-06\n",
      "It 07950: loss = 5.83113160e-06\n",
      "It 08000: loss = 3.41239320e-06\n",
      "It 08050: loss = 4.93478819e-06\n",
      "It 08100: loss = 4.54441533e-06\n",
      "It 08150: loss = 1.63903533e-05\n",
      "It 08200: loss = 4.86969766e-05\n",
      "It 08250: loss = 1.04751471e-05\n",
      "It 08300: loss = 4.62959633e-06\n",
      "It 08350: loss = 1.05707804e-05\n",
      "It 08400: loss = 1.52799112e-05\n",
      "It 08450: loss = 3.46811953e-06\n",
      "It 08500: loss = 6.94122036e-06\n",
      "It 08550: loss = 3.63034474e-06\n",
      "It 08600: loss = 5.53590962e-06\n",
      "It 08650: loss = 2.66602528e-05\n",
      "It 08700: loss = 3.54874351e-06\n",
      "It 08750: loss = 1.22202864e-05\n",
      "It 08800: loss = 3.50952791e-06\n",
      "It 08850: loss = 3.10245705e-05\n",
      "It 08900: loss = 4.63633614e-06\n",
      "It 08950: loss = 4.99811995e-06\n",
      "It 09000: loss = 1.26660771e-05\n",
      "It 09050: loss = 3.48276444e-06\n",
      "It 09100: loss = 1.30599910e-05\n",
      "It 09150: loss = 3.36188282e-06\n",
      "It 09200: loss = 8.87836723e-06\n",
      "It 09250: loss = 3.39384655e-06\n",
      "It 09300: loss = 5.60212357e-06\n",
      "It 09350: loss = 3.33610114e-06\n",
      "It 09400: loss = 1.04856645e-05\n",
      "It 09450: loss = 7.68868267e-06\n",
      "It 09500: loss = 9.43721898e-06\n",
      "It 09550: loss = 3.46985616e-05\n",
      "It 09600: loss = 4.99669386e-06\n",
      "It 09650: loss = 3.71588635e-06\n",
      "It 09700: loss = 5.07895356e-06\n",
      "It 09750: loss = 6.70880854e-06\n",
      "It 09800: loss = 1.40665197e-05\n",
      "It 09850: loss = 5.03450501e-05\n",
      "It 09900: loss = 3.88586841e-06\n",
      "It 09950: loss = 4.34495041e-06\n",
      "It 10000: loss = 1.62204669e-05\n",
      "It 10050: loss = 3.47332889e-06\n",
      "It 10100: loss = 5.21256015e-06\n",
      "It 10150: loss = 2.37831555e-05\n",
      "It 10200: loss = 3.66776408e-06\n",
      "It 10250: loss = 8.07701417e-06\n",
      "It 10300: loss = 3.58350417e-06\n",
      "It 10350: loss = 3.71056467e-06\n",
      "It 10400: loss = 4.00786448e-05\n",
      "It 10450: loss = 4.48183027e-06\n",
      "It 10500: loss = 4.83971999e-05\n",
      "It 10550: loss = 3.56146438e-06\n",
      "It 10600: loss = 1.68191109e-05\n",
      "It 10650: loss = 3.45663284e-06\n",
      "It 10700: loss = 9.28295685e-06\n",
      "It 10750: loss = 5.31826372e-05\n",
      "It 10800: loss = 3.72252362e-06\n",
      "It 10850: loss = 3.82097369e-06\n",
      "It 10900: loss = 1.33860813e-05\n",
      "It 10950: loss = 3.35928303e-06\n",
      "It 11000: loss = 6.52559356e-06\n",
      "It 11050: loss = 4.63900733e-06\n",
      "It 11100: loss = 4.53545590e-06\n",
      "It 11150: loss = 4.64070081e-06\n",
      "It 11200: loss = 6.21314102e-05\n",
      "It 11250: loss = 3.63583354e-06\n",
      "It 11300: loss = 5.48008502e-05\n",
      "It 11350: loss = 3.57104341e-06\n",
      "It 11400: loss = 1.11142917e-05\n",
      "It 11450: loss = 3.75246191e-06\n",
      "It 11500: loss = 3.73458738e-06\n",
      "It 11550: loss = 4.01201123e-06\n",
      "It 11600: loss = 3.04447058e-05\n",
      "It 11650: loss = 3.98468364e-06\n",
      "It 11700: loss = 9.19713602e-06\n",
      "It 11750: loss = 3.47997320e-06\n",
      "It 11800: loss = 4.67930295e-06\n",
      "It 11850: loss = 5.17981498e-05\n",
      "It 11900: loss = 3.62236915e-06\n",
      "It 11950: loss = 4.61098625e-06\n",
      "It 12000: loss = 6.08182891e-05\n",
      "It 12050: loss = 4.78410220e-06\n",
      "It 12100: loss = 5.22558867e-06\n",
      "It 12150: loss = 7.68607879e-06\n",
      "It 12200: loss = 4.44251964e-06\n",
      "It 12250: loss = 3.12258203e-06\n",
      "It 12300: loss = 8.66015853e-06\n",
      "It 12350: loss = 3.16860428e-06\n",
      "It 12400: loss = 6.39772770e-06\n",
      "It 12450: loss = 1.44388441e-05\n",
      "It 12500: loss = 5.08587709e-06\n",
      "It 12550: loss = 4.43567205e-06\n",
      "It 12600: loss = 3.72871227e-06\n",
      "It 12650: loss = 1.75142668e-05\n",
      "It 12700: loss = 3.80636538e-06\n",
      "It 12750: loss = 5.76636512e-06\n",
      "It 12800: loss = 9.49478999e-06\n",
      "It 12850: loss = 1.06727111e-05\n",
      "It 12900: loss = 5.41666759e-06\n",
      "It 12950: loss = 9.53333347e-06\n",
      "It 13000: loss = 3.10872133e-06\n",
      "It 13050: loss = 3.90874720e-06\n",
      "It 13100: loss = 3.06767265e-06\n",
      "It 13150: loss = 4.82235237e-06\n",
      "It 13200: loss = 4.31042026e-05\n",
      "It 13250: loss = 3.34913443e-06\n",
      "It 13300: loss = 2.97111892e-05\n",
      "It 13350: loss = 3.30260832e-06\n",
      "It 13400: loss = 1.05634226e-05\n",
      "It 13450: loss = 8.35563878e-06\n",
      "It 13500: loss = 4.27821033e-06\n",
      "It 13550: loss = 1.35932096e-05\n",
      "It 13600: loss = 3.23275481e-06\n",
      "It 13650: loss = 3.60684180e-06\n",
      "It 13700: loss = 4.59869225e-05\n",
      "It 13750: loss = 3.45635885e-06\n",
      "It 13800: loss = 5.49591932e-05\n",
      "It 13850: loss = 3.31877823e-06\n",
      "It 13900: loss = 6.58909630e-05\n",
      "It 13950: loss = 3.27147995e-06\n",
      "It 14000: loss = 3.68287920e-06\n",
      "It 14050: loss = 4.76941887e-06\n",
      "It 14100: loss = 7.18139199e-05\n",
      "It 14150: loss = 3.53968244e-06\n",
      "It 14200: loss = 4.86886893e-06\n",
      "It 14250: loss = 6.86541534e-06\n",
      "It 14300: loss = 3.25281781e-06\n",
      "It 14350: loss = 5.67209281e-06\n",
      "It 14400: loss = 9.87145540e-05\n",
      "It 14450: loss = 3.62569699e-06\n",
      "It 14500: loss = 9.39820438e-06\n",
      "It 14550: loss = 3.08944691e-05\n",
      "It 14600: loss = 3.93180017e-06\n",
      "It 14650: loss = 2.97671540e-05\n",
      "It 14700: loss = 3.26348118e-06\n",
      "It 14750: loss = 7.36640231e-06\n",
      "It 14800: loss = 4.39400283e-05\n",
      "It 14850: loss = 3.71735928e-06\n",
      "It 14900: loss = 5.09095025e-06\n",
      "It 14950: loss = 3.73722469e-05\n",
      "It 15000: loss = 3.47068203e-06\n",
      "It 15050: loss = 7.70134466e-06\n",
      "It 15100: loss = 5.49491142e-06\n",
      "It 15150: loss = 3.93906248e-06\n",
      "It 15200: loss = 6.03927765e-06\n",
      "It 15250: loss = 1.86917987e-05\n",
      "It 15300: loss = 1.23831414e-05\n",
      "It 15350: loss = 2.71312892e-05\n",
      "It 15400: loss = 2.39760957e-05\n",
      "It 15450: loss = 3.08414292e-06\n",
      "It 15500: loss = 6.56396605e-06\n",
      "It 15550: loss = 3.15864759e-06\n",
      "It 15600: loss = 4.65622998e-06\n",
      "It 15650: loss = 3.42570274e-05\n",
      "It 15700: loss = 3.21596895e-06\n",
      "It 15750: loss = 1.79652343e-05\n",
      "It 15800: loss = 2.03225536e-05\n",
      "It 15850: loss = 1.89051443e-05\n",
      "It 15900: loss = 5.17611534e-06\n",
      "It 15950: loss = 6.93831498e-06\n",
      "It 16000: loss = 2.09785649e-05\n",
      "It 16050: loss = 6.28399903e-06\n",
      "It 16100: loss = 1.67881371e-05\n",
      "It 16150: loss = 4.47717830e-05\n",
      "It 16200: loss = 7.50940762e-06\n",
      "It 16250: loss = 1.84250912e-05\n",
      "It 16300: loss = 5.99124178e-06\n",
      "It 16350: loss = 1.10435521e-05\n",
      "It 16400: loss = 2.34828785e-05\n",
      "It 16450: loss = 3.72771647e-05\n",
      "It 16500: loss = 3.16983505e-06\n",
      "It 16550: loss = 1.11587333e-05\n",
      "It 16600: loss = 4.76040077e-06\n",
      "It 16650: loss = 3.77166043e-06\n",
      "It 16700: loss = 2.58485816e-05\n",
      "It 16750: loss = 3.20462459e-06\n",
      "It 16800: loss = 1.45536487e-05\n",
      "It 16850: loss = 3.00932743e-06\n",
      "It 16900: loss = 6.14669443e-06\n",
      "It 16950: loss = 8.09893463e-05\n",
      "It 17000: loss = 2.93471317e-06\n",
      "It 17050: loss = 9.96079325e-06\n",
      "It 17100: loss = 3.14926706e-06\n",
      "It 17150: loss = 3.86659194e-05\n",
      "It 17200: loss = 3.20721188e-06\n",
      "It 17250: loss = 1.19585486e-04\n",
      "It 17300: loss = 3.40702513e-06\n",
      "It 17350: loss = 1.49432872e-05\n",
      "It 17400: loss = 3.09250777e-06\n",
      "It 17450: loss = 4.93327389e-06\n",
      "It 17500: loss = 2.20083166e-05\n",
      "It 17550: loss = 3.59990304e-06\n",
      "It 17600: loss = 4.45854266e-06\n",
      "It 17650: loss = 2.45716346e-05\n",
      "It 17700: loss = 3.02547232e-06\n",
      "It 17750: loss = 6.81771326e-06\n",
      "It 17800: loss = 3.62054816e-05\n",
      "It 17850: loss = 3.17098011e-06\n",
      "It 17900: loss = 7.25395785e-06\n",
      "It 17950: loss = 2.37610766e-05\n",
      "It 18000: loss = 3.52301345e-06\n",
      "It 18050: loss = 3.81117752e-06\n",
      "It 18100: loss = 3.65371707e-05\n",
      "It 18150: loss = 1.41134769e-05\n",
      "It 18200: loss = 4.57441547e-06\n",
      "It 18250: loss = 5.40938163e-06\n",
      "It 18300: loss = 2.20698330e-05\n",
      "It 18350: loss = 3.13063356e-06\n",
      "It 18400: loss = 3.48827098e-06\n",
      "It 18450: loss = 3.68255860e-06\n",
      "It 18500: loss = 1.71281663e-05\n",
      "It 18550: loss = 7.02166653e-06\n",
      "It 18600: loss = 3.91899903e-06\n",
      "It 18650: loss = 6.67456243e-06\n",
      "It 18700: loss = 1.65031397e-05\n",
      "It 18750: loss = 1.20993982e-05\n",
      "It 18800: loss = 9.02392003e-06\n",
      "It 18850: loss = 8.83003486e-06\n",
      "It 18900: loss = 1.99218666e-05\n",
      "It 18950: loss = 3.69392365e-06\n",
      "It 19000: loss = 3.75740387e-06\n",
      "It 19050: loss = 1.87448732e-05\n",
      "It 19100: loss = 3.35252707e-06\n",
      "It 19150: loss = 1.16459078e-05\n",
      "It 19200: loss = 5.37123706e-05\n",
      "It 19250: loss = 4.04341245e-06\n",
      "It 19300: loss = 4.94324649e-06\n",
      "It 19350: loss = 1.59829178e-05\n",
      "It 19400: loss = 2.68438853e-05\n",
      "It 19450: loss = 2.49780805e-05\n",
      "It 19500: loss = 1.93850010e-05\n",
      "It 19550: loss = 7.10782251e-06\n",
      "It 19600: loss = 4.34608000e-06\n",
      "It 19650: loss = 7.66875564e-06\n",
      "It 19700: loss = 7.80363735e-06\n",
      "It 19750: loss = 1.38637852e-05\n",
      "It 19800: loss = 2.18200184e-05\n",
      "It 19850: loss = 2.83624536e-06\n",
      "It 19900: loss = 3.77428842e-06\n",
      "It 19950: loss = 1.23731479e-05\n"
     ]
    }
   ],
   "source": [
    "k = int(20000)\n",
    "\n",
    "\n",
    "epochs =  k\n",
    "for iter in range(0,epochs): \n",
    "        with tf.GradientTape() as t:                 \n",
    "                u0_pred, v0_pred = net(q, t0_tf)\n",
    "                u_lb_pred, v_lb_pred, u_x_lb_pred, v_x_lb_pred = net_uv(x_lb_tf, t_lb_tf)\n",
    "                u_ub_pred, v_ub_pred, u_x_ub_pred, v_x_ub_pred = net_uv(x_ub_tf, t_ub_tf)\n",
    "                f_u_pred, f_v_pred = net_f_uv(x_f_tf, t_f_tf)\n",
    "                loss = Loss2(u0_pred, v0_pred,u_lb_pred, v_lb_pred, u_x_lb_pred, v_x_lb_pred,u_ub_pred, v_ub_pred, u_x_ub_pred, v_x_ub_pred,f_u_pred, f_v_pred)\n",
    "                grad = t.gradient(loss, train_vars)\n",
    "                optim.apply_gradients(zip(grad, train_vars))\n",
    "                train_loss_record2.append(loss)\n",
    "        if iter % 50 == 0:\n",
    "                print('It {:05d}: loss = {:10.8e}'.format(iter,loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "657d95d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T17:00:09.206215Z",
     "iopub.status.busy": "2025-02-06T17:00:09.205497Z",
     "iopub.status.idle": "2025-02-06T17:00:09.780516Z",
     "shell.execute_reply": "2025-02-06T17:00:09.779579Z"
    },
    "papermill": {
     "duration": 0.636409,
     "end_time": "2025-02-06T17:00:09.782377",
     "exception": false,
     "start_time": "2025-02-06T17:00:09.145968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#nn.saveweights\n",
    "nn.save_weights('/kaggle/working/nn2_3.weights.h5')\n",
    "np.savetxt('/kaggle/working/train_loss_record23.csv', train_loss_record2, fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b0e9554",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T17:00:09.867995Z",
     "iopub.status.busy": "2025-02-06T17:00:09.867604Z",
     "iopub.status.idle": "2025-02-06T17:07:18.136649Z",
     "shell.execute_reply": "2025-02-06T17:07:18.135193Z"
    },
    "papermill": {
     "duration": 428.314119,
     "end_time": "2025-02-06T17:07:18.138742",
     "exception": false,
     "start_time": "2025-02-06T17:00:09.824623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-27-6015997f0176>:108: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  func = function_factory(nn,Loss2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Iter: 1 loss: 7.32094122e-06\n",
      "Iter: 2 loss: 0.108545162\n",
      "Iter: 3 loss: 4.12513873e-06\n",
      "Iter: 4 loss: 3.47259765e-06\n",
      "Iter: 5 loss: 7.79828952e-06\n",
      "Iter: 6 loss: 3.40277211e-06\n",
      "Iter: 7 loss: 3.10423866e-06\n",
      "Iter: 8 loss: 3.07279652e-06\n",
      "Iter: 9 loss: 2.8538052e-06\n",
      "Iter: 10 loss: 2.78933476e-06\n",
      "Iter: 11 loss: 2.82280757e-06\n",
      "Iter: 12 loss: 2.74661033e-06\n",
      "Iter: 13 loss: 2.72063789e-06\n",
      "Iter: 14 loss: 2.72058378e-06\n",
      "Iter: 15 loss: 2.6942696e-06\n",
      "Iter: 16 loss: 2.7653366e-06\n",
      "Iter: 17 loss: 2.68565327e-06\n",
      "Iter: 18 loss: 2.6783473e-06\n",
      "Iter: 19 loss: 2.70208238e-06\n",
      "Iter: 20 loss: 2.67631981e-06\n",
      "Iter: 21 loss: 2.6736443e-06\n",
      "Iter: 22 loss: 2.67091605e-06\n",
      "Iter: 23 loss: 2.67032e-06\n",
      "Iter: 24 loss: 2.66744155e-06\n",
      "Iter: 25 loss: 2.66072425e-06\n",
      "Iter: 26 loss: 2.74138824e-06\n",
      "Iter: 27 loss: 2.66022e-06\n",
      "Iter: 28 loss: 2.65547465e-06\n",
      "Iter: 29 loss: 2.65378367e-06\n",
      "Iter: 30 loss: 2.65119252e-06\n",
      "Iter: 31 loss: 2.64796427e-06\n",
      "Iter: 32 loss: 2.64790469e-06\n",
      "Iter: 33 loss: 2.64492451e-06\n",
      "Iter: 34 loss: 2.65544691e-06\n",
      "Iter: 35 loss: 2.6441719e-06\n",
      "Iter: 36 loss: 2.64337791e-06\n",
      "Iter: 37 loss: 2.64238452e-06\n",
      "Iter: 38 loss: 2.64232699e-06\n",
      "Iter: 39 loss: 2.64127652e-06\n",
      "Iter: 40 loss: 2.64172809e-06\n",
      "Iter: 41 loss: 2.64057053e-06\n",
      "Iter: 42 loss: 2.63994093e-06\n",
      "Iter: 43 loss: 2.63884544e-06\n",
      "Iter: 44 loss: 2.63879178e-06\n",
      "Iter: 45 loss: 2.63825791e-06\n",
      "Iter: 46 loss: 2.6394257e-06\n",
      "Iter: 47 loss: 2.6380651e-06\n",
      "Iter: 48 loss: 2.63757943e-06\n",
      "Iter: 49 loss: 2.63796096e-06\n",
      "Iter: 50 loss: 2.63737115e-06\n",
      "Iter: 51 loss: 2.6369953e-06\n",
      "Iter: 52 loss: 2.63620291e-06\n",
      "Iter: 53 loss: 2.64235723e-06\n",
      "Iter: 54 loss: 2.63600373e-06\n",
      "Iter: 55 loss: 2.63535298e-06\n",
      "Iter: 56 loss: 2.6350915e-06\n",
      "Iter: 57 loss: 2.63475181e-06\n",
      "Iter: 58 loss: 2.63450875e-06\n",
      "Iter: 59 loss: 2.634883e-06\n",
      "Iter: 60 loss: 2.63443258e-06\n",
      "Iter: 61 loss: 2.63433049e-06\n",
      "Iter: 62 loss: 2.63395987e-06\n",
      "Iter: 63 loss: 2.63334277e-06\n",
      "Iter: 64 loss: 2.63331458e-06\n",
      "Iter: 65 loss: 2.63237803e-06\n",
      "Iter: 66 loss: 2.63038305e-06\n",
      "Iter: 67 loss: 2.66442862e-06\n",
      "Iter: 68 loss: 2.63036304e-06\n",
      "Iter: 69 loss: 2.62915137e-06\n",
      "Iter: 70 loss: 2.62850949e-06\n",
      "Iter: 71 loss: 2.62791059e-06\n",
      "Iter: 72 loss: 2.62699791e-06\n",
      "Iter: 73 loss: 2.62551271e-06\n",
      "Iter: 74 loss: 2.62542358e-06\n",
      "Iter: 75 loss: 2.62475851e-06\n",
      "Iter: 76 loss: 2.62241247e-06\n",
      "Iter: 77 loss: 2.62295111e-06\n",
      "Iter: 78 loss: 2.62012031e-06\n",
      "Iter: 79 loss: 2.61720152e-06\n",
      "Iter: 80 loss: 2.61984633e-06\n",
      "Iter: 81 loss: 2.61557534e-06\n",
      "Iter: 82 loss: 2.61366e-06\n",
      "Iter: 83 loss: 2.60724528e-06\n",
      "Iter: 84 loss: 2.6056764e-06\n",
      "Iter: 85 loss: 2.60012484e-06\n",
      "Iter: 86 loss: 2.58942077e-06\n",
      "Iter: 87 loss: 2.56751332e-06\n",
      "Iter: 88 loss: 2.96783355e-06\n",
      "Iter: 89 loss: 2.56704652e-06\n",
      "Iter: 90 loss: 2.55409668e-06\n",
      "Iter: 91 loss: 2.60425395e-06\n",
      "Iter: 92 loss: 2.55077384e-06\n",
      "Iter: 93 loss: 2.54777069e-06\n",
      "Iter: 94 loss: 2.55456939e-06\n",
      "Iter: 95 loss: 2.54670067e-06\n",
      "Iter: 96 loss: 2.54347651e-06\n",
      "Iter: 97 loss: 2.54136398e-06\n",
      "Iter: 98 loss: 2.5401348e-06\n",
      "Iter: 99 loss: 2.53352641e-06\n",
      "Iter: 100 loss: 2.53405256e-06\n",
      "Iter: 101 loss: 2.52825362e-06\n",
      "Iter: 102 loss: 2.52528525e-06\n",
      "Iter: 103 loss: 2.52823133e-06\n",
      "Iter: 104 loss: 2.523582e-06\n",
      "Iter: 105 loss: 2.52046198e-06\n",
      "Iter: 106 loss: 2.51637789e-06\n",
      "Iter: 107 loss: 2.5160914e-06\n",
      "Iter: 108 loss: 2.51339884e-06\n",
      "Iter: 109 loss: 2.51735878e-06\n",
      "Iter: 110 loss: 2.51220285e-06\n",
      "Iter: 111 loss: 2.50957487e-06\n",
      "Iter: 112 loss: 2.50261428e-06\n",
      "Iter: 113 loss: 2.54980796e-06\n",
      "Iter: 114 loss: 2.50102926e-06\n",
      "Iter: 115 loss: 2.49344112e-06\n",
      "Iter: 116 loss: 2.48357787e-06\n",
      "Iter: 117 loss: 2.48280116e-06\n",
      "Iter: 118 loss: 2.47156481e-06\n",
      "Iter: 119 loss: 2.50475568e-06\n",
      "Iter: 120 loss: 2.46853915e-06\n",
      "Iter: 121 loss: 2.45550973e-06\n",
      "Iter: 122 loss: 2.48213837e-06\n",
      "Iter: 123 loss: 2.44949229e-06\n",
      "Iter: 124 loss: 2.44006105e-06\n",
      "Iter: 125 loss: 2.44230296e-06\n",
      "Iter: 126 loss: 2.43362638e-06\n",
      "Iter: 127 loss: 2.4285946e-06\n",
      "Iter: 128 loss: 2.44747912e-06\n",
      "Iter: 129 loss: 2.42739293e-06\n",
      "Iter: 130 loss: 2.42561578e-06\n",
      "Iter: 131 loss: 2.42808233e-06\n",
      "Iter: 132 loss: 2.42450255e-06\n",
      "Iter: 133 loss: 2.41903444e-06\n",
      "Iter: 134 loss: 2.41258385e-06\n",
      "Iter: 135 loss: 2.41196608e-06\n",
      "Iter: 136 loss: 2.40116424e-06\n",
      "Iter: 137 loss: 2.4339995e-06\n",
      "Iter: 138 loss: 2.39763244e-06\n",
      "Iter: 139 loss: 2.39181031e-06\n",
      "Iter: 140 loss: 2.42196393e-06\n",
      "Iter: 141 loss: 2.39110864e-06\n",
      "Iter: 142 loss: 2.3862633e-06\n",
      "Iter: 143 loss: 2.40741474e-06\n",
      "Iter: 144 loss: 2.38525831e-06\n",
      "Iter: 145 loss: 2.38321786e-06\n",
      "Iter: 146 loss: 2.38162283e-06\n",
      "Iter: 147 loss: 2.38089251e-06\n",
      "Iter: 148 loss: 2.37518634e-06\n",
      "Iter: 149 loss: 2.36892379e-06\n",
      "Iter: 150 loss: 2.36773803e-06\n",
      "Iter: 151 loss: 2.36261371e-06\n",
      "Iter: 152 loss: 2.38772577e-06\n",
      "Iter: 153 loss: 2.36167421e-06\n",
      "Iter: 154 loss: 2.35826224e-06\n",
      "Iter: 155 loss: 2.3627108e-06\n",
      "Iter: 156 loss: 2.35608832e-06\n",
      "Iter: 157 loss: 2.34693243e-06\n",
      "Iter: 158 loss: 2.3576431e-06\n",
      "Iter: 159 loss: 2.34209369e-06\n",
      "Iter: 160 loss: 2.33611877e-06\n",
      "Iter: 161 loss: 2.34256277e-06\n",
      "Iter: 162 loss: 2.33264086e-06\n",
      "Iter: 163 loss: 2.32534785e-06\n",
      "Iter: 164 loss: 2.31765057e-06\n",
      "Iter: 165 loss: 2.31624949e-06\n",
      "Iter: 166 loss: 2.30662795e-06\n",
      "Iter: 167 loss: 2.30170144e-06\n",
      "Iter: 168 loss: 2.2969557e-06\n",
      "Iter: 169 loss: 2.27575038e-06\n",
      "Iter: 170 loss: 2.34431218e-06\n",
      "Iter: 171 loss: 2.26967359e-06\n",
      "Iter: 172 loss: 2.26042175e-06\n",
      "Iter: 173 loss: 2.26019074e-06\n",
      "Iter: 174 loss: 2.25834469e-06\n",
      "Iter: 175 loss: 2.26198404e-06\n",
      "Iter: 176 loss: 2.25763051e-06\n",
      "Iter: 177 loss: 2.25608278e-06\n",
      "Iter: 178 loss: 2.25366671e-06\n",
      "Iter: 179 loss: 2.25362646e-06\n",
      "Iter: 180 loss: 2.24866517e-06\n",
      "Iter: 181 loss: 2.25222061e-06\n",
      "Iter: 182 loss: 2.24565247e-06\n",
      "Iter: 183 loss: 2.23353231e-06\n",
      "Iter: 184 loss: 2.44672356e-06\n",
      "Iter: 185 loss: 2.23232405e-06\n",
      "Iter: 186 loss: 2.21173741e-06\n",
      "Iter: 187 loss: 3.27377438e-06\n",
      "Iter: 188 loss: 2.21163828e-06\n",
      "Iter: 189 loss: 2.17481534e-06\n",
      "Iter: 190 loss: 2.53354e-06\n",
      "Iter: 191 loss: 2.17354705e-06\n",
      "Iter: 192 loss: 2.15942532e-06\n",
      "Iter: 193 loss: 2.26566931e-06\n",
      "Iter: 194 loss: 2.15817158e-06\n",
      "Iter: 195 loss: 2.15490013e-06\n",
      "Iter: 196 loss: 2.16022431e-06\n",
      "Iter: 197 loss: 2.15300315e-06\n",
      "Iter: 198 loss: 2.15134901e-06\n",
      "Iter: 199 loss: 2.16363355e-06\n",
      "Iter: 200 loss: 2.15122691e-06\n",
      "Iter: 201 loss: 2.15037244e-06\n",
      "Iter: 202 loss: 2.14908232e-06\n",
      "Iter: 203 loss: 2.14917577e-06\n",
      "Iter: 204 loss: 2.14456213e-06\n",
      "Iter: 205 loss: 2.13314866e-06\n",
      "Iter: 206 loss: 2.25746112e-06\n",
      "Iter: 207 loss: 2.13154317e-06\n",
      "Iter: 208 loss: 2.12888767e-06\n",
      "Iter: 209 loss: 2.15392333e-06\n",
      "Iter: 210 loss: 2.12877103e-06\n",
      "Iter: 211 loss: 2.12797431e-06\n",
      "Iter: 212 loss: 2.13157114e-06\n",
      "Iter: 213 loss: 2.12789e-06\n",
      "Iter: 214 loss: 2.12669534e-06\n",
      "Iter: 215 loss: 2.12342161e-06\n",
      "Iter: 216 loss: 2.13893077e-06\n",
      "Iter: 217 loss: 2.12212353e-06\n",
      "Iter: 218 loss: 2.11643692e-06\n",
      "Iter: 219 loss: 2.11710767e-06\n",
      "Iter: 220 loss: 2.11216275e-06\n",
      "Iter: 221 loss: 2.10489657e-06\n",
      "Iter: 222 loss: 2.13481871e-06\n",
      "Iter: 223 loss: 2.10334974e-06\n",
      "Iter: 224 loss: 2.10142275e-06\n",
      "Iter: 225 loss: 2.13165322e-06\n",
      "Iter: 226 loss: 2.10139433e-06\n",
      "Iter: 227 loss: 2.09981363e-06\n",
      "Iter: 228 loss: 2.09653558e-06\n",
      "Iter: 229 loss: 2.16713465e-06\n",
      "Iter: 230 loss: 2.09644281e-06\n",
      "Iter: 231 loss: 2.09037421e-06\n",
      "Iter: 232 loss: 2.07623134e-06\n",
      "Iter: 233 loss: 2.28091267e-06\n",
      "Iter: 234 loss: 2.07518269e-06\n",
      "Iter: 235 loss: 2.06966479e-06\n",
      "Iter: 236 loss: 2.11237261e-06\n",
      "Iter: 237 loss: 2.06915479e-06\n",
      "Iter: 238 loss: 2.06895425e-06\n",
      "Iter: 239 loss: 2.06894447e-06\n",
      "Iter: 240 loss: 2.06871073e-06\n",
      "Iter: 241 loss: 2.07086509e-06\n",
      "Iter: 242 loss: 2.06870027e-06\n",
      "Iter: 243 loss: 2.06851814e-06\n",
      "Iter: 244 loss: 2.06772893e-06\n",
      "Iter: 245 loss: 2.06897084e-06\n",
      "Iter: 246 loss: 2.06725667e-06\n",
      "Iter: 247 loss: 2.06549157e-06\n",
      "Iter: 248 loss: 2.06186724e-06\n",
      "Iter: 249 loss: 2.12435612e-06\n",
      "Iter: 250 loss: 2.0618736e-06\n",
      "Iter: 251 loss: 2.05760557e-06\n",
      "Iter: 252 loss: 2.06518735e-06\n",
      "Iter: 253 loss: 2.05543097e-06\n",
      "Iter: 254 loss: 2.05392143e-06\n",
      "Iter: 255 loss: 2.05385936e-06\n",
      "Iter: 256 loss: 2.05337892e-06\n",
      "Iter: 257 loss: 2.0575178e-06\n",
      "Iter: 258 loss: 2.05336391e-06\n",
      "Iter: 259 loss: 2.05323067e-06\n",
      "Iter: 260 loss: 2.05374636e-06\n",
      "Iter: 261 loss: 2.05324727e-06\n",
      "Iter: 262 loss: 2.05294987e-06\n",
      "Iter: 263 loss: 2.05192055e-06\n",
      "Iter: 264 loss: 2.04907246e-06\n",
      "Iter: 265 loss: 2.08999154e-06\n",
      "Iter: 266 loss: 2.0488726e-06\n",
      "Iter: 267 loss: 2.04192224e-06\n",
      "Iter: 268 loss: 2.02389037e-06\n",
      "Iter: 269 loss: 2.17261868e-06\n",
      "Iter: 270 loss: 2.02076171e-06\n",
      "Iter: 271 loss: 2.01803823e-06\n",
      "Iter: 272 loss: 2.0278012e-06\n",
      "Iter: 273 loss: 2.01718194e-06\n",
      "Iter: 274 loss: 2.01600142e-06\n",
      "Iter: 275 loss: 2.01837156e-06\n",
      "Iter: 276 loss: 2.01552052e-06\n",
      "Iter: 277 loss: 2.01269859e-06\n",
      "Iter: 278 loss: 2.01714192e-06\n",
      "Iter: 279 loss: 2.01090484e-06\n",
      "Iter: 280 loss: 2.00618979e-06\n",
      "Iter: 281 loss: 2.01383364e-06\n",
      "Iter: 282 loss: 2.00371483e-06\n",
      "Iter: 283 loss: 1.99894566e-06\n",
      "Iter: 284 loss: 2.01723151e-06\n",
      "Iter: 285 loss: 1.99846704e-06\n",
      "Iter: 286 loss: 1.99411193e-06\n",
      "Iter: 287 loss: 2.02270053e-06\n",
      "Iter: 288 loss: 1.9935851e-06\n",
      "Iter: 289 loss: 1.99298506e-06\n",
      "Iter: 290 loss: 1.99586202e-06\n",
      "Iter: 291 loss: 1.99284068e-06\n",
      "Iter: 292 loss: 1.99248552e-06\n",
      "Iter: 293 loss: 1.99108581e-06\n",
      "Iter: 294 loss: 1.99219085e-06\n",
      "Iter: 295 loss: 1.98994235e-06\n",
      "Iter: 296 loss: 1.98443217e-06\n",
      "Iter: 297 loss: 1.96744213e-06\n",
      "Iter: 298 loss: 2.00049681e-06\n",
      "Iter: 299 loss: 1.95656685e-06\n",
      "Iter: 300 loss: 1.9511067e-06\n",
      "Iter: 301 loss: 1.96450037e-06\n",
      "Iter: 302 loss: 1.94921245e-06\n",
      "Iter: 303 loss: 1.94505878e-06\n",
      "Iter: 304 loss: 1.93390724e-06\n",
      "Iter: 305 loss: 2.00514592e-06\n",
      "Iter: 306 loss: 1.93111214e-06\n",
      "Iter: 307 loss: 1.91586264e-06\n",
      "Iter: 308 loss: 1.98159546e-06\n",
      "Iter: 309 loss: 1.91332629e-06\n",
      "Iter: 310 loss: 1.90641379e-06\n",
      "Iter: 311 loss: 1.90610024e-06\n",
      "Iter: 312 loss: 1.9045865e-06\n",
      "Iter: 313 loss: 1.91249501e-06\n",
      "Iter: 314 loss: 1.90434798e-06\n",
      "Iter: 315 loss: 1.90350886e-06\n",
      "Iter: 316 loss: 1.9038921e-06\n",
      "Iter: 317 loss: 1.90295782e-06\n",
      "Iter: 318 loss: 1.90241212e-06\n",
      "Iter: 319 loss: 1.90478e-06\n",
      "Iter: 320 loss: 1.90233857e-06\n",
      "Iter: 321 loss: 1.90094931e-06\n",
      "Iter: 322 loss: 1.89978198e-06\n",
      "Iter: 323 loss: 1.89948275e-06\n",
      "Iter: 324 loss: 1.89583693e-06\n",
      "Iter: 325 loss: 1.89079071e-06\n",
      "Iter: 326 loss: 1.89046068e-06\n",
      "Iter: 327 loss: 1.88799595e-06\n",
      "Iter: 328 loss: 1.91107074e-06\n",
      "Iter: 329 loss: 1.88779632e-06\n",
      "Iter: 330 loss: 1.88688261e-06\n",
      "Iter: 331 loss: 1.88819752e-06\n",
      "Iter: 332 loss: 1.8863899e-06\n",
      "Iter: 333 loss: 1.88512138e-06\n",
      "Iter: 334 loss: 1.88236436e-06\n",
      "Iter: 335 loss: 1.95060761e-06\n",
      "Iter: 336 loss: 1.88229194e-06\n",
      "Iter: 337 loss: 1.87589399e-06\n",
      "Iter: 338 loss: 1.96759379e-06\n",
      "Iter: 339 loss: 1.87584465e-06\n",
      "Iter: 340 loss: 1.8615342e-06\n",
      "Iter: 341 loss: 1.87847468e-06\n",
      "Iter: 342 loss: 1.8539705e-06\n",
      "Iter: 343 loss: 1.84849966e-06\n",
      "Iter: 344 loss: 1.84840735e-06\n",
      "Iter: 345 loss: 1.84763132e-06\n",
      "Iter: 346 loss: 1.8567531e-06\n",
      "Iter: 347 loss: 1.84762473e-06\n",
      "Iter: 348 loss: 1.84701059e-06\n",
      "Iter: 349 loss: 1.85007548e-06\n",
      "Iter: 350 loss: 1.84687883e-06\n",
      "Iter: 351 loss: 1.84609883e-06\n",
      "Iter: 352 loss: 1.8491944e-06\n",
      "Iter: 353 loss: 1.8459267e-06\n",
      "Iter: 354 loss: 1.84464352e-06\n",
      "Iter: 355 loss: 1.84180021e-06\n",
      "Iter: 356 loss: 1.87820183e-06\n",
      "Iter: 357 loss: 1.84157921e-06\n",
      "Iter: 358 loss: 1.83620898e-06\n",
      "Iter: 359 loss: 1.82172107e-06\n",
      "Iter: 360 loss: 2.26843758e-06\n",
      "Iter: 361 loss: 1.82074757e-06\n",
      "Iter: 362 loss: 1.8015611e-06\n",
      "Iter: 363 loss: 1.80158077e-06\n",
      "Iter: 364 loss: 1.78788684e-06\n",
      "Iter: 365 loss: 1.77869561e-06\n",
      "Iter: 366 loss: 1.75918831e-06\n",
      "Iter: 367 loss: 1.75876471e-06\n",
      "Iter: 368 loss: 1.74514605e-06\n",
      "Iter: 369 loss: 1.74468266e-06\n",
      "Iter: 370 loss: 1.73534681e-06\n",
      "Iter: 371 loss: 1.73370813e-06\n",
      "Iter: 372 loss: 1.72692091e-06\n",
      "Iter: 373 loss: 1.72224145e-06\n",
      "Iter: 374 loss: 1.75832326e-06\n",
      "Iter: 375 loss: 1.72174316e-06\n",
      "Iter: 376 loss: 1.72015837e-06\n",
      "Iter: 377 loss: 1.73740875e-06\n",
      "Iter: 378 loss: 1.72018599e-06\n",
      "Iter: 379 loss: 1.71943475e-06\n",
      "Iter: 380 loss: 1.72087232e-06\n",
      "Iter: 381 loss: 1.71907266e-06\n",
      "Iter: 382 loss: 1.71812542e-06\n",
      "Iter: 383 loss: 1.71696604e-06\n",
      "Iter: 384 loss: 1.71679653e-06\n",
      "Iter: 385 loss: 1.71324098e-06\n",
      "Iter: 386 loss: 1.71618035e-06\n",
      "Iter: 387 loss: 1.71128249e-06\n",
      "Iter: 388 loss: 1.70628584e-06\n",
      "Iter: 389 loss: 1.69942211e-06\n",
      "Iter: 390 loss: 1.69919645e-06\n",
      "Iter: 391 loss: 1.68925408e-06\n",
      "Iter: 392 loss: 1.79097515e-06\n",
      "Iter: 393 loss: 1.68902477e-06\n",
      "Iter: 394 loss: 1.6823609e-06\n",
      "Iter: 395 loss: 1.74164074e-06\n",
      "Iter: 396 loss: 1.68142071e-06\n",
      "Iter: 397 loss: 1.67920678e-06\n",
      "Iter: 398 loss: 1.70549561e-06\n",
      "Iter: 399 loss: 1.67925964e-06\n",
      "Iter: 400 loss: 1.67863243e-06\n",
      "Iter: 401 loss: 1.68405484e-06\n",
      "Iter: 402 loss: 1.67867688e-06\n",
      "Iter: 403 loss: 1.67839869e-06\n",
      "Iter: 404 loss: 1.67969677e-06\n",
      "Iter: 405 loss: 1.67828864e-06\n",
      "Iter: 406 loss: 1.6780408e-06\n",
      "Iter: 407 loss: 1.67764983e-06\n",
      "Iter: 408 loss: 1.67755729e-06\n",
      "Iter: 409 loss: 1.67697942e-06\n",
      "Iter: 410 loss: 1.67533392e-06\n",
      "Iter: 411 loss: 1.69311204e-06\n",
      "Iter: 412 loss: 1.67510859e-06\n",
      "Iter: 413 loss: 1.67229382e-06\n",
      "Iter: 414 loss: 1.66766381e-06\n",
      "Iter: 415 loss: 1.66761708e-06\n",
      "Iter: 416 loss: 1.6606574e-06\n",
      "Iter: 417 loss: 1.65175959e-06\n",
      "Iter: 418 loss: 1.65103768e-06\n",
      "Iter: 419 loss: 1.64251901e-06\n",
      "Iter: 420 loss: 1.6395154e-06\n",
      "Iter: 421 loss: 1.63479046e-06\n",
      "Iter: 422 loss: 1.62373306e-06\n",
      "Iter: 423 loss: 1.67612382e-06\n",
      "Iter: 424 loss: 1.62007098e-06\n",
      "Iter: 425 loss: 1.61184789e-06\n",
      "Iter: 426 loss: 1.61111564e-06\n",
      "Iter: 427 loss: 1.60808918e-06\n",
      "Iter: 428 loss: 1.69108876e-06\n",
      "Iter: 429 loss: 1.60814261e-06\n",
      "Iter: 430 loss: 1.6068285e-06\n",
      "Iter: 431 loss: 1.61286891e-06\n",
      "Iter: 432 loss: 1.60654724e-06\n",
      "Iter: 433 loss: 1.60589582e-06\n",
      "Iter: 434 loss: 1.60667878e-06\n",
      "Iter: 435 loss: 1.60564616e-06\n",
      "Iter: 436 loss: 1.60517641e-06\n",
      "Iter: 437 loss: 1.60632749e-06\n",
      "Iter: 438 loss: 1.6050717e-06\n",
      "Iter: 439 loss: 1.6049562e-06\n",
      "Iter: 440 loss: 1.60536342e-06\n",
      "Iter: 441 loss: 1.60483933e-06\n",
      "Iter: 442 loss: 1.60462741e-06\n",
      "Iter: 443 loss: 1.60478282e-06\n",
      "Iter: 444 loss: 1.60451805e-06\n",
      "Iter: 445 loss: 1.60439095e-06\n",
      "Iter: 446 loss: 1.60426566e-06\n",
      "Iter: 447 loss: 1.60425066e-06\n",
      "Iter: 448 loss: 1.60377238e-06\n",
      "Iter: 449 loss: 1.60292802e-06\n",
      "Iter: 450 loss: 1.61844093e-06\n",
      "Iter: 451 loss: 1.60283366e-06\n",
      "Iter: 452 loss: 1.60134925e-06\n",
      "Iter: 453 loss: 1.60022387e-06\n",
      "Iter: 454 loss: 1.59983767e-06\n",
      "Iter: 455 loss: 1.59638785e-06\n",
      "Iter: 456 loss: 1.59202182e-06\n",
      "Iter: 457 loss: 1.59178455e-06\n",
      "Iter: 458 loss: 1.58637056e-06\n",
      "Iter: 459 loss: 1.58249793e-06\n",
      "Iter: 460 loss: 1.58047624e-06\n",
      "Iter: 461 loss: 1.57094144e-06\n",
      "Iter: 462 loss: 1.62945605e-06\n",
      "Iter: 463 loss: 1.56965086e-06\n",
      "Iter: 464 loss: 1.56460067e-06\n",
      "Iter: 465 loss: 1.58466526e-06\n",
      "Iter: 466 loss: 1.56354565e-06\n",
      "Iter: 467 loss: 1.56177362e-06\n",
      "Iter: 468 loss: 1.56959629e-06\n",
      "Iter: 469 loss: 1.56125043e-06\n",
      "Iter: 470 loss: 1.56010128e-06\n",
      "Iter: 471 loss: 1.55984264e-06\n",
      "Iter: 472 loss: 1.55912858e-06\n",
      "Iter: 473 loss: 1.57152488e-06\n",
      "Iter: 474 loss: 1.55909458e-06\n",
      "Iter: 475 loss: 1.55890643e-06\n",
      "Iter: 476 loss: 1.55950602e-06\n",
      "Iter: 477 loss: 1.55878956e-06\n",
      "Iter: 478 loss: 1.55861153e-06\n",
      "Iter: 479 loss: 1.55838268e-06\n",
      "Iter: 480 loss: 1.55832777e-06\n",
      "Iter: 481 loss: 1.55779696e-06\n",
      "Iter: 482 loss: 1.55612349e-06\n",
      "Iter: 483 loss: 1.56287672e-06\n",
      "Iter: 484 loss: 1.55555449e-06\n",
      "Iter: 485 loss: 1.55280566e-06\n",
      "Iter: 486 loss: 1.548917e-06\n",
      "Iter: 487 loss: 1.54883151e-06\n",
      "Iter: 488 loss: 1.54302381e-06\n",
      "Iter: 489 loss: 1.54930808e-06\n",
      "Iter: 490 loss: 1.5388423e-06\n",
      "Iter: 491 loss: 1.52661357e-06\n",
      "Iter: 492 loss: 1.62868173e-06\n",
      "Iter: 493 loss: 1.52626546e-06\n",
      "Iter: 494 loss: 1.51468498e-06\n",
      "Iter: 495 loss: 1.51474444e-06\n",
      "Iter: 496 loss: 1.49853963e-06\n",
      "Iter: 497 loss: 1.56340241e-06\n",
      "Iter: 498 loss: 1.49489267e-06\n",
      "Iter: 499 loss: 1.48114941e-06\n",
      "Iter: 500 loss: 1.50953053e-06\n",
      "Iter: 501 loss: 1.47461446e-06\n",
      "Iter: 502 loss: 1.46387629e-06\n",
      "Iter: 503 loss: 1.55369014e-06\n",
      "Iter: 504 loss: 1.46347577e-06\n",
      "Iter: 505 loss: 1.45837305e-06\n",
      "Iter: 506 loss: 1.49782409e-06\n",
      "Iter: 507 loss: 1.45792023e-06\n",
      "Iter: 508 loss: 1.45453419e-06\n",
      "Iter: 509 loss: 1.4729535e-06\n",
      "Iter: 510 loss: 1.45383262e-06\n",
      "Iter: 511 loss: 1.45159675e-06\n",
      "Iter: 512 loss: 1.46162006e-06\n",
      "Iter: 513 loss: 1.45109857e-06\n",
      "Iter: 514 loss: 1.44944136e-06\n",
      "Iter: 515 loss: 1.45424121e-06\n",
      "Iter: 516 loss: 1.44893841e-06\n",
      "Iter: 517 loss: 1.44827584e-06\n",
      "Iter: 518 loss: 1.45180366e-06\n",
      "Iter: 519 loss: 1.4482365e-06\n",
      "Iter: 520 loss: 1.44787555e-06\n",
      "Iter: 521 loss: 1.44869705e-06\n",
      "Iter: 522 loss: 1.44778687e-06\n",
      "Iter: 523 loss: 1.44743876e-06\n",
      "Iter: 524 loss: 1.44763931e-06\n",
      "Iter: 525 loss: 1.4471882e-06\n",
      "Iter: 526 loss: 1.44671708e-06\n",
      "Iter: 527 loss: 1.44585727e-06\n",
      "Iter: 528 loss: 1.46471712e-06\n",
      "Iter: 529 loss: 1.44592661e-06\n",
      "Iter: 530 loss: 1.44423109e-06\n",
      "Iter: 531 loss: 1.44030298e-06\n",
      "Iter: 532 loss: 1.49388825e-06\n",
      "Iter: 533 loss: 1.44001115e-06\n",
      "Iter: 534 loss: 1.43387899e-06\n",
      "Iter: 535 loss: 1.42840236e-06\n",
      "Iter: 536 loss: 1.42678255e-06\n",
      "Iter: 537 loss: 1.41250234e-06\n",
      "Iter: 538 loss: 1.49497396e-06\n",
      "Iter: 539 loss: 1.41041778e-06\n",
      "Iter: 540 loss: 1.40071052e-06\n",
      "Iter: 541 loss: 1.44286457e-06\n",
      "Iter: 542 loss: 1.39869621e-06\n",
      "Iter: 543 loss: 1.39443398e-06\n",
      "Iter: 544 loss: 1.41114924e-06\n",
      "Iter: 545 loss: 1.3934166e-06\n",
      "Iter: 546 loss: 1.39311987e-06\n",
      "Iter: 547 loss: 1.39298879e-06\n",
      "Iter: 548 loss: 1.39267195e-06\n",
      "Iter: 549 loss: 1.39266308e-06\n",
      "Iter: 550 loss: 1.39247186e-06\n",
      "Iter: 551 loss: 1.39258839e-06\n",
      "Iter: 552 loss: 1.39230622e-06\n",
      "Iter: 553 loss: 1.39209567e-06\n",
      "Iter: 554 loss: 1.39172323e-06\n",
      "Iter: 555 loss: 1.39167503e-06\n",
      "Iter: 556 loss: 1.39084091e-06\n",
      "Iter: 557 loss: 1.38806854e-06\n",
      "Iter: 558 loss: 1.39684346e-06\n",
      "Iter: 559 loss: 1.38677694e-06\n",
      "Iter: 560 loss: 1.38013547e-06\n",
      "Iter: 561 loss: 1.37672873e-06\n",
      "Iter: 562 loss: 1.3736269e-06\n",
      "Iter: 563 loss: 1.36613176e-06\n",
      "Iter: 564 loss: 1.44961382e-06\n",
      "Iter: 565 loss: 1.36599169e-06\n",
      "Iter: 566 loss: 1.3630405e-06\n",
      "Iter: 567 loss: 1.37752454e-06\n",
      "Iter: 568 loss: 1.36249946e-06\n",
      "Iter: 569 loss: 1.36211679e-06\n",
      "Iter: 570 loss: 1.36512529e-06\n",
      "Iter: 571 loss: 1.36221161e-06\n",
      "Iter: 572 loss: 1.36214487e-06\n",
      "Iter: 573 loss: 1.36225287e-06\n",
      "Iter: 574 loss: 1.36210519e-06\n",
      "Iter: 575 loss: 1.36204517e-06\n",
      "Iter: 576 loss: 1.36216579e-06\n",
      "Iter: 577 loss: 1.36208837e-06\n",
      "Iter: 578 loss: 1.36201857e-06\n",
      "Iter: 579 loss: 1.36180722e-06\n",
      "Iter: 580 loss: 1.36229073e-06\n",
      "Iter: 581 loss: 1.36169751e-06\n",
      "Iter: 582 loss: 1.36110395e-06\n",
      "Iter: 583 loss: 1.35970151e-06\n",
      "Iter: 584 loss: 1.3754161e-06\n",
      "Iter: 585 loss: 1.35957691e-06\n",
      "Iter: 586 loss: 1.35568109e-06\n",
      "Iter: 587 loss: 1.3467245e-06\n",
      "Iter: 588 loss: 1.49038692e-06\n",
      "Iter: 589 loss: 1.34618074e-06\n",
      "Iter: 590 loss: 1.33739195e-06\n",
      "Iter: 591 loss: 1.3644983e-06\n",
      "Iter: 592 loss: 1.33485548e-06\n",
      "Iter: 593 loss: 1.33395179e-06\n",
      "Iter: 594 loss: 1.33395406e-06\n",
      "Iter: 595 loss: 1.33386379e-06\n",
      "Iter: 596 loss: 1.33462311e-06\n",
      "Iter: 597 loss: 1.33382468e-06\n",
      "Iter: 598 loss: 1.33387448e-06\n",
      "Iter: 599 loss: 1.3338705e-06\n",
      "Iter: 600 loss: 1.33385493e-06\n",
      "Iter: 601 loss: 1.33387903e-06\n",
      "Iter: 602 loss: 1.33388539e-06\n",
      "Iter: 603 loss: 1.33388312e-06\n",
      "Iter: 604 loss: 1.33383094e-06\n",
      "Iter: 605 loss: 1.33379433e-06\n",
      "Iter: 606 loss: 1.33378046e-06\n",
      "Iter: 607 loss: 1.33384674e-06\n",
      "Iter: 608 loss: 1.33383526e-06\n",
      "Iter: 609 loss: 1.33380763e-06\n",
      "Iter: 610 loss: 1.33383242e-06\n",
      "Iter: 611 loss: 1.33382184e-06\n",
      "Iter: 612 loss: 1.33383412e-06\n",
      "Iter: 613 loss: 1.33384788e-06\n",
      "Iter: 614 loss: 1.33384287e-06\n",
      "Iter: 615 loss: 1.33382753e-06\n",
      "Iter: 616 loss: 1.33382696e-06\n",
      "Iter: 617 loss: 1.33382696e-06\n",
      "Iter: 618 loss: 1.33382184e-06\n",
      "Iter: 619 loss: 1.3338215e-06\n",
      "Iter: 620 loss: 1.3338215e-06\n",
      "Iter: 621 loss: 1.3338215e-06\n",
      "Iter: 622 loss: 1.33382696e-06\n",
      "Iter: 623 loss: 1.33382696e-06\n",
      "Iter: 624 loss: 1.3338215e-06\n",
      "Iter: 625 loss: 1.33382696e-06\n",
      "Iter: 626 loss: 1.33382696e-06\n",
      "Iter: 627 loss: 1.33382696e-06\n",
      "Iter: 628 loss: 1.33382696e-06\n",
      "Iter: 629 loss: 1.3338215e-06\n",
      "Iter: 630 loss: 1.33428341e-06\n",
      "Iter: 631 loss: 1.33388e-06\n",
      "Iter: 632 loss: 1.33383935e-06\n",
      "Iter: 633 loss: 1.333819e-06\n",
      "Iter: 634 loss: 1.33385583e-06\n",
      "Iter: 635 loss: 1.33386357e-06\n",
      "Iter: 636 loss: 1.33383412e-06\n",
      "Iter: 637 loss: 1.33380286e-06\n",
      "Iter: 638 loss: 1.33377762e-06\n",
      "Iter: 639 loss: 1.33376466e-06\n",
      "Iter: 640 loss: 1.33386e-06\n",
      "Iter: 641 loss: 1.33382957e-06\n",
      "Iter: 642 loss: 1.33386311e-06\n",
      "Iter: 643 loss: 1.3338398e-06\n",
      "Iter: 644 loss: 1.33382343e-06\n",
      "Iter: 645 loss: 1.33379899e-06\n",
      "Iter: 646 loss: 1.33381707e-06\n",
      "Iter: 647 loss: 1.33379854e-06\n",
      "Iter: 648 loss: 1.33383071e-06\n",
      "Iter: 649 loss: 1.33379854e-06\n",
      "Iter: 650 loss: 1.33382503e-06\n",
      "Iter: 651 loss: 1.33381309e-06\n",
      "Iter: 652 loss: 1.33381172e-06\n",
      "Iter: 653 loss: 1.33381309e-06\n",
      "Iter: 654 loss: 1.33382514e-06\n",
      "Iter: 655 loss: 1.33381309e-06\n",
      "Iter: 656 loss: 1.33382537e-06\n",
      "Iter: 657 loss: 1.33381309e-06\n",
      "Iter: 658 loss: 1.33382537e-06\n",
      "Iter: 659 loss: 1.33381309e-06\n",
      "Iter: 660 loss: 1.33381309e-06\n",
      "Iter: 661 loss: 1.33382537e-06\n",
      "Iter: 662 loss: 1.33383514e-06\n",
      "Iter: 663 loss: 1.33385356e-06\n",
      "Iter: 664 loss: 1.33382343e-06\n",
      "Iter: 665 loss: 1.33382548e-06\n",
      "Iter: 666 loss: 1.33383094e-06\n",
      "Iter: 667 loss: 1.33380377e-06\n",
      "Iter: 668 loss: 1.33380729e-06\n",
      "Iter: 669 loss: 1.33382491e-06\n",
      "Iter: 670 loss: 1.33383742e-06\n",
      "Iter: 671 loss: 1.33377603e-06\n",
      "Iter: 672 loss: 1.33385674e-06\n",
      "Iter: 673 loss: 1.33382161e-06\n",
      "Iter: 674 loss: 1.33382309e-06\n",
      "Iter: 675 loss: 1.33379831e-06\n",
      "Iter: 676 loss: 1.33382036e-06\n",
      "Iter: 677 loss: 1.33380968e-06\n",
      "Iter: 678 loss: 1.33382764e-06\n",
      "Iter: 679 loss: 1.33382525e-06\n",
      "Iter: 680 loss: 1.3338298e-06\n",
      "Iter: 681 loss: 1.33382684e-06\n",
      "Iter: 682 loss: 1.3338115e-06\n",
      "Iter: 683 loss: 1.33381309e-06\n",
      "Iter: 684 loss: 1.33381263e-06\n",
      "Iter: 685 loss: 1.33381263e-06\n",
      "Iter: 686 loss: 1.33381263e-06\n",
      "Iter: 687 loss: 1.33381263e-06\n",
      "Iter: 688 loss: 1.33382684e-06\n",
      "Iter: 689 loss: 1.33381263e-06\n",
      "Iter: 690 loss: 1.33381263e-06\n"
     ]
    }
   ],
   "source": [
    "#nn = neural_net(components)\n",
    "\"\"\"An example of using tfp.optimizer.lbfgs_minimize to optimize a TensorFlow z.\n",
    "This code shows a naive way to wrap a tf.keras.z and optimize it with the L-BFGS\n",
    "optimizer from TensorFlow Probability.\n",
    "Python interpreter version: 3.6.9\n",
    "TensorFlow version: 2.0.0\n",
    "TensorFlow Probability version: 0.8.0\n",
    "NumPy version: 1.17.2\n",
    "Matplotlib version: 3.1.1\n",
    "\"\"\"\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "def function_factory(z, loss):# train_x, train_y):\n",
    "    \"\"\"A factory to create a function required by tfp.optimizer.lbfgs_minimize.\n",
    "    Args:\n",
    "        z [in]: an instance of `tf.keras.z` or its subclasses.\n",
    "        loss [in]: a function with signature loss_value = loss(pred_y, true_y).\n",
    "        train_x [in]: the input part of training data.\n",
    "        train_y [in]: the output part of training data.\n",
    "    Returns:\n",
    "        A function that has a signature of:\n",
    "            loss_value, gradients = f(z_parameters).\n",
    "    \"\"\"\n",
    "\n",
    "    # obtain the shapes of all trainable parameters in the z\n",
    "    shapes = tf.shape_n(z.trainable_variables)\n",
    "    n_tensors = len(shapes)\n",
    "\n",
    "    # we'll use tf.dynamic_stitch and tf.dynamic_partition later, so we need to\n",
    "    # prepare required information first\n",
    "    count = 0\n",
    "    idx = [] # stitch indices\n",
    "    part = [] # partition indices\n",
    "\n",
    "    for i, shape in enumerate(shapes):\n",
    "        n = np.product(shape)\n",
    "        idx.append(tf.reshape(tf.range(count, count+n, dtype=tf.int32), shape))\n",
    "        part.extend([i]*n)\n",
    "        count += n\n",
    "\n",
    "    part = tf.constant(part)\n",
    "\n",
    "    @tf.function\n",
    "    def assign_new_z_parameters(params_1d):\n",
    "        \"\"\"A function updating the z's parameters with a 1D tf.Tensor.\n",
    "        Args:\n",
    "            params_1d [in]: a 1D tf.Tensor representing the z's trainable parameters.\n",
    "        \"\"\"\n",
    "\n",
    "        params = tf.dynamic_partition(params_1d, part, n_tensors)\n",
    "        for i, (shape, param) in enumerate(zip(shapes, params)):\n",
    "            z.trainable_variables[i].assign(tf.reshape(param, shape))\n",
    "\n",
    "    # now create a function that will be returned by this factory\n",
    "    @tf.function\n",
    "    def f(params_1d):\n",
    "        \"\"\"A function that can be used by tfp.optimizer.lbfgs_minimize.\n",
    "        This function is created by function_factory.\n",
    "        Args:\n",
    "           params_1d [in]: a 1D tf.Tensor.\n",
    "        Returns:\n",
    "            A scalar loss and the gradients w.r.t. the `params_1d`.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as t: \n",
    "                #t.watch(train_vars)\n",
    "            assign_new_z_parameters(params_1d)\n",
    "            u0_pred, v0_pred,b,a = net_uv(q, t0_tf)\n",
    "            u_lb_pred, v_lb_pred, u_x_lb_pred, v_x_lb_pred = net_uv(x_lb_tf, t_lb_tf)\n",
    "            u_ub_pred, v_ub_pred, u_x_ub_pred, v_x_ub_pred = net_uv(x_ub_tf, t_ub_tf)\n",
    "            f_u_pred, f_v_pred = net_f_uv(x_f_tf, t_f_tf)\n",
    "            loss_value = Loss2(u0_pred, v0_pred,u_lb_pred, v_lb_pred, u_x_lb_pred, v_x_lb_pred,u_ub_pred, v_ub_pred, u_x_ub_pred, v_x_ub_pred,f_u_pred, f_v_pred)\n",
    "            grads = t.gradient(loss_value, z.trainable_variables)\n",
    "\n",
    "        # calculate gradients and convert to 1D tf.Tensor\n",
    "        #grads = tape.gradient(loss_value, z.trainable_variables)\n",
    "        print(type(grads[0]))\n",
    "        grads = tf.dynamic_stitch(idx, grads)\n",
    "       \n",
    "\n",
    "        # print out iteration & loss\n",
    "        f.iter.assign_add(1)\n",
    "        tf.print(\"Iter:\", f.iter, \"loss:\", loss_value)\n",
    "\n",
    "        # store loss value so we can retrieve later\n",
    "        tf.py_function(f.history.append, inp=[loss_value], Tout=[])\n",
    "\n",
    "        return loss_value, grads\n",
    "\n",
    "    # store these information as members so we can use them outside the scope\n",
    "    f.iter = tf.Variable(0)\n",
    "    f.idx = idx\n",
    "    f.part = part\n",
    "    f.shapes = shapes\n",
    "    f.assign_new_z_parameters = assign_new_z_parameters\n",
    "    f.history = []\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "# use float64 by default\n",
    "tf.keras.backend.set_floatx(\"float64\")\n",
    "\n",
    "\n",
    "func = function_factory(nn,Loss2)\n",
    "\n",
    "# convert initial z parameters to a 1D tf.Tensor\n",
    "init_params = tf.dynamic_stitch(func.idx, nn.trainable_variables)\n",
    "\n",
    "# train the z with L-BFGS solver\n",
    "results = tfp.optimizer.lbfgs_minimize(value_and_gradients_function=func, initial_position=init_params,max_iterations=10000,f_absolute_tolerance=1.0 * np.finfo(float).eps,max_line_search_iterations=100,num_correction_pairs=100)\n",
    "#tfp.optimizer.lbfgs_minimize(\n",
    "    # value_and_gradients_function=func, initial_position=init_params, max_iterations=1000)\n",
    "\n",
    "# after training, the final optimized parameters are still in results.position\n",
    "# so we have to manually put them back to the z\n",
    "func.assign_new_z_parameters(results.position)\n",
    "l3 =func.history \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9901ab9",
   "metadata": {
    "papermill": {
     "duration": 0.075224,
     "end_time": "2025-02-06T17:07:18.290218",
     "exception": false,
     "start_time": "2025-02-06T17:07:18.214994",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "optim.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c946f0d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T17:07:18.448898Z",
     "iopub.status.busy": "2025-02-06T17:07:18.448517Z",
     "iopub.status.idle": "2025-02-06T17:07:18.531842Z",
     "shell.execute_reply": "2025-02-06T17:07:18.531038Z"
    },
    "papermill": {
     "duration": 0.16571,
     "end_time": "2025-02-06T17:07:18.533614",
     "exception": false,
     "start_time": "2025-02-06T17:07:18.367904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "store = {}\n",
    "optim.save_own_variables(store)\n",
    "u = []\n",
    "\n",
    "i = 0\n",
    "for a in store.values():\n",
    "    \n",
    "    u.append(a)\n",
    "\n",
    "    k = str(i)\n",
    "    if np.shape(a) == ():\n",
    "        a = [a]\n",
    "\n",
    "    \n",
    "    np.savetxt('/kaggle/working/var'+str(i)+'.csv',a)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3919db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T17:15:16.860131Z",
     "iopub.status.busy": "2025-01-02T17:15:16.859785Z",
     "iopub.status.idle": "2025-01-02T17:15:16.86435Z",
     "shell.execute_reply": "2025-01-02T17:15:16.863133Z",
     "shell.execute_reply.started": "2025-01-02T17:15:16.8601Z"
    },
    "papermill": {
     "duration": 0.07665,
     "end_time": "2025-02-06T17:07:18.687809",
     "exception": false,
     "start_time": "2025-02-06T17:07:18.611159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3807cdfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T17:07:18.842805Z",
     "iopub.status.busy": "2025-02-06T17:07:18.842452Z",
     "iopub.status.idle": "2025-02-06T17:07:18.861888Z",
     "shell.execute_reply": "2025-02-06T17:07:18.861109Z"
    },
    "papermill": {
     "duration": 0.099173,
     "end_time": "2025-02-06T17:07:18.863757",
     "exception": false,
     "start_time": "2025-02-06T17:07:18.764584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#nn.saveweights\n",
    "nn.save_weights('/kaggle/working/nn2_4.weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "025e2bc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T17:07:19.020224Z",
     "iopub.status.busy": "2025-02-06T17:07:19.019725Z",
     "iopub.status.idle": "2025-02-06T17:07:19.030068Z",
     "shell.execute_reply": "2025-02-06T17:07:19.029164Z"
    },
    "papermill": {
     "duration": 0.090706,
     "end_time": "2025-02-06T17:07:19.031796",
     "exception": false,
     "start_time": "2025-02-06T17:07:18.941090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "np.savetxt('/kaggle/working/l3.csv', l3, fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55d1e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_v = nn.predict(X_star)\n",
    "u_pred, v_pred = u_v[:,0:1],u_v[:,1:2]\n",
    "f_u_pred,f_v_pred = net_f_uv(tf.constant(X_star[:,0:1],dtype=\"float32\"),tf.constant(X_star[:,1:2],dtype=\"float32\"))\n",
    "u_v = nn.predict(X_star)\n",
    "#u_pred, v_pred = u_v[:,0:1],u_v[:,1:2]\n",
    "#f_u_pred,f_v_pred = net_f_uv(tf.constant(X_star[:,0:1],float64),tf.constant(X_star[:,1:2]))\n",
    "h_pred = np.sqrt(u_pred**2 + v_pred**2)\n",
    "        \n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "error_v = np.linalg.norm(v_star-v_pred,2)/np.linalg.norm(v_star,2)\n",
    "error_h = np.linalg.norm(h_star-h_pred,2)/np.linalg.norm(h_star,2)\n",
    "print('Error u: %e' % (error_u))\n",
    "print('Error v: %e' % (error_v))\n",
    "print('Error h: %e' % (error_h))\n",
    "\n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "V_pred = griddata(X_star, v_pred.flatten(), (X, T), method='cubic')\n",
    "H_pred = griddata(X_star, h_pred.flatten(), (X, T), method='cubic')\n",
    "\n",
    "FU_pred = griddata(X_star, np.array(f_u_pred).flatten(), (X, T), method='cubic')\n",
    "FV_pred = griddata(X_star, np.array(f_v_pred).flatten(), (X, T), method='cubic')\n",
    "gs1 = gridspec.GridSpec(1, 3)\n",
    "gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 0])\n",
    "ax.plot(x,Exact_h[:,75], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x,H_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$|h(t,x)|$')    \n",
    "#ax.set_title('$t = %.2f$' % (t[75]), fontsize = 10)\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-5.1,5.1])\n",
    "ax.set_ylim([-0.1,5.1])\n",
    "\n",
    "ax = plt.subplot(gs1[0, 1])\n",
    "ax.plot(x,Exact_h[:,100], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x,H_pred[100,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$|h(t,x)|$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-5.1,5.1])\n",
    "ax.set_ylim([-0.1,5.1])\n",
    "#ax.set_title('$t = %.2f$' % (t[100]), fontsize = 10)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.8), ncol=5, frameon=False)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 2])\n",
    "ax.plot(x,Exact_h[:,125], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x,H_pred[125,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$|h(t,x)|$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-5.1,5.1])\n",
    "ax.set_ylim([-0.1,5.1])    \n",
    "#ax.set_title('$t = %.2f$' % (t[125]), fontsize = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3bc1e707",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T17:07:19.196749Z",
     "iopub.status.busy": "2025-02-06T17:07:19.196376Z",
     "iopub.status.idle": "2025-02-06T17:07:19.229195Z",
     "shell.execute_reply": "2025-02-06T17:07:19.228210Z"
    },
    "papermill": {
     "duration": 0.121325,
     "end_time": "2025-02-06T17:07:19.231904",
     "exception": false,
     "start_time": "2025-02-06T17:07:19.110579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Variable path=adam/iteration, shape=(), dtype=int64, value=110000>, <Variable path=adam/learning_rate, shape=(), dtype=float32, value=0.0005000000237487257>, <Variable path=adam/sequential_dense_10_kernel_momentum, shape=(2, 100), dtype=float32, value=[[ 3.28252281e-05  9.63115308e-05  2.50287439e-06 -6.80262383e-06\n",
      "   1.02628746e-06  1.46050206e-05  4.91247338e-05 -1.31520301e-05\n",
      "  -1.30934772e-04  5.09803112e-06 -1.02543083e-04 -1.38909127e-05\n",
      "  -8.38030155e-06 -1.10134361e-05 -1.27672363e-04  2.34871604e-05\n",
      "   2.98500163e-05 -1.48473609e-05  2.66538227e-05  5.87035265e-06\n",
      "   9.14622524e-06  2.46833770e-06 -6.30258637e-06 -6.83240796e-06\n",
      "  -5.03954689e-05 -1.55812195e-05  3.42585408e-05 -1.39101280e-06\n",
      "   1.65819602e-05 -1.04586052e-05  3.13399614e-05  3.76259982e-06\n",
      "   2.34197523e-05  2.09244990e-05  5.98834413e-06 -2.01980129e-05\n",
      "   2.27919736e-05 -1.93855121e-05 -1.73297485e-05 -1.81546829e-06\n",
      "  -1.48034324e-05 -1.15945504e-05  7.11422326e-05  6.50011680e-06\n",
      "   5.69777330e-06 -3.50593764e-05 -2.41821363e-05 -4.76061214e-05\n",
      "  -2.25634576e-05  1.49320240e-05  2.86844388e-05 -3.03735687e-05\n",
      "   1.80391157e-06 -1.88132490e-05 -2.03615527e-05  1.37917705e-05\n",
      "  -2.49020104e-07 -1.16832041e-06  1.65923557e-05  5.12675324e-05\n",
      "  -2.58186665e-05  4.57066062e-06 -3.32222662e-05  7.09199503e-06\n",
      "   5.69905660e-06 -1.30338412e-05  1.18173230e-05 -1.30379340e-05\n",
      "   1.30008211e-05 -7.22732602e-06 -7.47805025e-05  1.80065672e-05\n",
      "  -3.59450132e-05 -5.59753926e-06 -3.08398303e-05 -1.32169871e-05\n",
      "  -1.99822779e-07 -1.97208683e-05  1.72444106e-05 -6.79553523e-06\n",
      "   1.30180852e-04 -2.23143106e-05 -1.57410323e-05  1.58146759e-05\n",
      "  -5.16084356e-06  3.00601823e-06 -4.12510190e-06  7.98073324e-06\n",
      "  -4.48648207e-05 -1.11319205e-05 -7.92483115e-06 -1.08361215e-04\n",
      "   2.03577838e-05  1.07336484e-06 -4.85529017e-06 -3.12141274e-05\n",
      "  -8.94406985e-05 -8.53699203e-06  2.59673343e-05  2.84234773e-07]\n",
      " [-1.66654068e-08 -1.67456406e-07 -5.30837108e-09  4.27227320e-09\n",
      "  -4.18788515e-09 -3.13584430e-08 -1.03589784e-07  7.56671703e-09\n",
      "   2.44671668e-07  9.99639926e-10  1.77194082e-07 -1.25559563e-09\n",
      "   1.98928536e-08  1.61436855e-08  2.15901494e-07 -3.41166242e-08\n",
      "  -1.36026497e-08 -1.03760911e-09 -4.00939975e-08 -7.90045718e-09\n",
      "  -1.01477262e-08 -5.97531091e-09 -2.92545721e-09 -1.04046345e-08\n",
      "   3.45729916e-08  3.70926045e-08 -3.75210547e-08  7.31343341e-09\n",
      "  -3.04860537e-09  1.72636820e-08 -3.08232302e-08 -1.44468562e-08\n",
      "  -1.41866670e-08 -8.43510595e-09 -3.81762000e-09  3.72192162e-08\n",
      "  -2.89584428e-08  1.66327663e-08  1.19169208e-08 -6.32171204e-09\n",
      "  -3.95988664e-09  3.56027785e-09 -7.38837898e-08 -8.12403833e-09\n",
      "  -6.30796704e-09  2.08482227e-08  3.48273304e-08  9.84548407e-08\n",
      "   5.17831609e-08  3.73815912e-09 -2.34739197e-08  2.39418938e-08\n",
      "   9.70672787e-09  3.71326081e-08  3.06961780e-08 -8.93446028e-09\n",
      "  -1.07680229e-08 -9.51191303e-09 -1.46334944e-09 -1.11060814e-07\n",
      "   1.87674711e-08 -1.62459504e-08  7.60077938e-08 -8.55829541e-09\n",
      "   1.14101395e-08  1.24046347e-08  8.24062596e-09  4.38703154e-08\n",
      "  -6.73935530e-09  6.43987219e-09  7.69788997e-08 -2.22201955e-08\n",
      "   2.32441515e-08 -1.28185489e-08  1.11441771e-08  8.40654568e-09\n",
      "   2.51335242e-09  7.91233568e-09 -2.25258945e-09  1.06758273e-08\n",
      "  -1.72221903e-07  7.49828999e-09  2.85779187e-08 -1.58418008e-08\n",
      "   1.48543045e-09  1.41917145e-08 -1.11312168e-08 -3.09500159e-09\n",
      "   4.27484359e-08  6.09313133e-09 -4.00106703e-09  1.88975875e-07\n",
      "  -2.02905355e-08 -1.22471189e-08 -2.19978546e-09  1.44576520e-08\n",
      "   1.33598689e-07 -1.52084620e-08 -1.06696234e-08 -1.20670967e-08]]>, <Variable path=adam/sequential_dense_10_kernel_velocity, shape=(2, 100), dtype=float32, value=[[3.40061689e-07 3.29998602e-06 3.81958962e-08 1.10193746e-07\n",
      "  7.99844599e-08 2.63097292e-07 1.38460268e-06 5.33770574e-08\n",
      "  7.31386172e-06 1.70197190e-08 4.86351792e-06 3.49145495e-08\n",
      "  3.69065383e-08 6.29871550e-08 7.01056706e-06 3.17758435e-07\n",
      "  2.93437552e-07 7.82914285e-08 1.88262575e-07 4.89225123e-08\n",
      "  3.06414982e-08 6.36906847e-08 1.81930382e-08 2.60310120e-08\n",
      "  9.89380396e-07 1.62329101e-07 5.35771903e-07 1.46952210e-08\n",
      "  5.24574446e-08 8.87267291e-08 4.89474985e-07 5.94307501e-08\n",
      "  1.00192615e-07 1.18482291e-07 1.17405685e-08 2.12717055e-07\n",
      "  3.53510814e-07 1.44392587e-07 1.13846916e-07 9.92558657e-09\n",
      "  5.46775993e-08 8.18572161e-08 1.56785688e-06 4.25197513e-08\n",
      "  2.74329626e-08 3.06166839e-07 3.72660992e-07 1.02393369e-06\n",
      "  3.16498273e-07 1.04010162e-07 2.22749080e-07 3.08020276e-07\n",
      "  3.86438970e-08 2.96591992e-07 2.76943041e-07 1.36748511e-07\n",
      "  1.85849842e-08 3.46515563e-08 1.74863274e-07 1.61673904e-06\n",
      "  1.80812933e-07 6.30448937e-08 6.95503900e-07 5.47604699e-08\n",
      "  1.88688745e-08 4.56353817e-08 1.70280984e-07 1.87671219e-07\n",
      "  8.97568242e-08 4.93400307e-08 1.62092181e-06 1.02187492e-07\n",
      "  3.17917198e-07 1.81934308e-08 2.67818791e-07 3.25231753e-08\n",
      "  7.80723841e-09 1.51112218e-07 8.20118800e-08 1.70633569e-07\n",
      "  5.29680028e-06 9.61088489e-08 1.60647602e-07 5.43939258e-08\n",
      "  1.24755610e-08 4.65137902e-08 2.59463295e-08 4.47876403e-08\n",
      "  6.27276393e-07 1.32717105e-07 1.37780205e-08 4.76149762e-06\n",
      "  2.36742721e-07 1.11560556e-08 3.79960241e-08 3.98638974e-07\n",
      "  3.67607754e-06 3.71377347e-08 1.63418164e-07 3.62533825e-08]\n",
      " [4.71015642e-14 1.11653698e-11 1.25677694e-13 1.53270747e-14\n",
      "  4.52367026e-14 7.53990024e-13 5.96095655e-12 7.87540131e-14\n",
      "  2.83323764e-11 3.12865409e-14 1.73205946e-11 1.70724962e-14\n",
      "  2.63398461e-13 8.96924340e-14 2.37862958e-11 8.32154278e-13\n",
      "  2.18295705e-14 3.89281672e-14 4.59861261e-13 2.13727809e-14\n",
      "  1.71106771e-14 6.14688964e-14 5.00361742e-14 2.63699137e-13\n",
      "  6.55896073e-13 4.42768272e-13 1.50227844e-12 3.65758517e-14\n",
      "  8.50726892e-14 1.16082437e-13 1.10199892e-12 7.58738707e-14\n",
      "  1.30407797e-13 1.18974917e-14 8.30354801e-14 1.05305456e-12\n",
      "  6.65314212e-13 6.05040649e-14 8.79021316e-14 1.04347615e-14\n",
      "  2.34082349e-14 2.94152615e-13 1.91265797e-12 9.01073243e-14\n",
      "  3.48619618e-14 1.00609069e-13 7.01993205e-13 3.68966186e-12\n",
      "  9.39771481e-13 6.63323563e-14 2.37108049e-13 8.24107113e-13\n",
      "  1.92331014e-13 5.74071227e-13 4.28807163e-13 2.53517368e-13\n",
      "  5.06482876e-14 2.55476060e-13 1.82506815e-13 5.10357251e-12\n",
      "  5.23509865e-14 1.31535557e-13 2.73431191e-12 3.71381935e-14\n",
      "  7.64415454e-14 3.12944047e-14 5.27257600e-13 8.60663049e-13\n",
      "  2.63541576e-13 1.19676269e-13 1.96189831e-12 1.27943622e-13\n",
      "  1.76233052e-13 1.90479387e-13 1.78414277e-14 2.11420762e-14\n",
      "  6.20485049e-15 1.17141512e-13 2.69768915e-14 1.40760288e-13\n",
      "  1.10114487e-11 7.85848115e-15 5.17365231e-13 5.26917309e-14\n",
      "  3.08109014e-14 1.27134712e-13 8.37496237e-14 1.14571086e-13\n",
      "  5.72989139e-13 5.82435372e-14 2.73716986e-14 1.58252942e-11\n",
      "  1.48575149e-13 6.72770894e-14 9.10714324e-15 1.35839392e-13\n",
      "  1.08676792e-11 3.56493508e-13 1.74337592e-14 8.04810645e-13]]>, <Variable path=adam/sequential_dense_10_bias_momentum, shape=(100,), dtype=float32, value=[ 2.60398556e-05  9.98482210e-05 -3.04174137e-06  1.17267189e-06\n",
      "  7.68531572e-06  5.84231111e-06  4.40523218e-05 -1.64469366e-05\n",
      " -1.21213321e-04  7.53189033e-06 -8.54683603e-05 -1.24466278e-05\n",
      " -6.47834531e-06 -8.53387974e-06 -1.09263114e-04  1.32398263e-05\n",
      "  2.19128360e-05 -1.00230545e-05  2.98283085e-05  1.28673519e-05\n",
      "  9.79299421e-06  8.87501847e-06 -1.63054779e-06 -8.12978851e-06\n",
      " -3.37350903e-05 -2.08727106e-05  2.07851463e-05 -5.08692892e-06\n",
      "  1.86241341e-05 -1.66518475e-05  1.60768468e-05  4.72674401e-06\n",
      "  1.93503274e-05  1.58143357e-05  8.03546754e-06 -1.35082710e-05\n",
      "  1.24411581e-05 -1.70862204e-05 -1.03346210e-05  8.76615900e-08\n",
      " -1.00783918e-05 -4.45542355e-06  5.80189590e-05  3.83087172e-06\n",
      "  6.83873441e-06 -2.93392222e-05 -1.47576866e-05 -5.28679448e-05\n",
      " -2.68729127e-05  6.82187965e-06  2.17369361e-05 -1.91824911e-05\n",
      "  3.77899778e-06 -1.63426394e-05 -1.83156189e-05  5.51163293e-06\n",
      "  4.38744428e-06 -3.97332997e-06  4.06288018e-06  4.85556375e-05\n",
      " -2.23691932e-05  5.07974300e-06 -3.20435793e-05  3.77254719e-06\n",
      "  3.31755655e-06 -1.79405342e-05  2.98588304e-07 -1.64714911e-05\n",
      "  6.81455276e-06 -7.20999310e-07 -6.46514818e-05  2.35904918e-05\n",
      " -2.69897573e-05 -7.13539430e-06 -2.24543146e-05 -1.19994374e-05\n",
      " -8.50386073e-07 -1.03385728e-05  1.19263923e-05  4.02827800e-06\n",
      "  1.13975744e-04 -1.83743559e-05 -1.14210543e-05  1.63777986e-05\n",
      " -6.90770958e-06 -2.18887408e-06  3.32294576e-07  1.26607702e-05\n",
      " -3.78233890e-05 -3.12083739e-06 -8.86979069e-06 -9.74059803e-05\n",
      "  1.51325585e-05  2.12919963e-06  1.66467248e-06 -2.01980420e-05\n",
      " -6.93490001e-05 -2.99249155e-06  2.34702347e-05 -4.79326627e-06]>, <Variable path=adam/sequential_dense_10_bias_velocity, shape=(100,), dtype=float32, value=[1.20876010e-07 1.51249003e-06 2.92641182e-08 3.98192661e-08\n",
      " 6.10946600e-08 6.45434568e-08 3.89612950e-07 1.41132361e-07\n",
      " 2.21748587e-06 5.76681174e-08 1.17593584e-06 1.18067334e-07\n",
      " 1.11910960e-08 2.06607726e-08 1.81545033e-06 3.70899258e-08\n",
      " 8.43447623e-08 6.70661962e-08 1.68838014e-07 6.20678691e-08\n",
      " 2.03937081e-08 4.29839986e-08 1.80902653e-08 9.97918619e-08\n",
      " 1.95957355e-07 7.40332240e-08 6.77140690e-08 6.44641762e-09\n",
      " 1.46081064e-07 6.65441462e-08 4.39014620e-08 4.47622490e-08\n",
      " 1.56342978e-07 5.46453194e-08 2.34552910e-08 3.37026940e-08\n",
      " 3.93042683e-08 4.54351152e-08 1.62716915e-08 1.56993192e-08\n",
      " 6.50085923e-08 3.74282365e-08 5.33442574e-07 7.28313099e-09\n",
      " 4.89834058e-08 1.74427583e-07 6.93536677e-08 4.21780953e-07\n",
      " 1.31273097e-07 4.82481717e-08 1.00000790e-07 8.97404746e-08\n",
      " 6.43390266e-08 9.90326114e-08 7.65831203e-08 6.95478475e-09\n",
      " 6.53098864e-09 3.51459803e-08 4.37765699e-08 4.95665802e-07\n",
      " 8.87117793e-08 5.38321174e-08 2.24758764e-07 1.79743953e-08\n",
      " 5.09416331e-08 7.55706466e-08 1.68995253e-08 8.59277165e-08\n",
      " 1.65042593e-08 1.18610854e-09 6.59225918e-07 1.36329547e-07\n",
      " 1.75390880e-07 7.15499908e-08 1.14784036e-07 5.80244475e-08\n",
      " 5.57459146e-09 5.79359494e-08 7.81506415e-08 9.06233026e-08\n",
      " 1.96118617e-06 1.16452298e-07 5.06847684e-08 7.02628000e-08\n",
      " 2.74677934e-08 4.81188245e-08 1.16364660e-08 9.05573714e-08\n",
      " 2.29283671e-07 3.28630350e-08 4.86492659e-08 1.42898625e-06\n",
      " 4.08053324e-08 1.01076543e-08 1.19162742e-08 7.00716569e-08\n",
      " 7.45724435e-07 5.71799808e-09 1.43091540e-07 3.82113505e-08]>, <Variable path=adam/sequential_dense_11_kernel_momentum, shape=(100, 100), dtype=float32, value=[[-9.8376017e-07  2.5247962e-07  1.1990787e-06 ...  1.1807680e-07\n",
      "   2.9288407e-07 -5.9453736e-08]\n",
      " [-8.9530012e-08  1.2524829e-07 -3.6937656e-08 ... -8.1944989e-08\n",
      "  -3.6349093e-08  8.9812815e-09]\n",
      " [ 6.7569511e-07 -3.5502231e-07 -6.6667417e-07 ...  1.7384946e-07\n",
      "  -6.8458291e-08 -5.7181651e-08]\n",
      " ...\n",
      " [-1.8904149e-09  9.5309787e-08 -5.3814617e-08 ... -1.3439937e-07\n",
      "  -1.9207850e-08  7.2003914e-08]\n",
      " [-8.5028478e-07  2.4974364e-07  9.8034559e-07 ...  6.9492003e-08\n",
      "   2.0214145e-07 -5.7492908e-08]\n",
      " [ 2.2192475e-07 -1.6340493e-07 -1.6478243e-07 ...  1.1978311e-07\n",
      "  -2.1093634e-09 -3.4983628e-08]]>, <Variable path=adam/sequential_dense_11_kernel_velocity, shape=(100, 100), dtype=float32, value=[[3.2675843e-10 3.3160344e-10 1.8785595e-09 ... 1.1562171e-09\n",
      "  1.0027229e-10 3.2180369e-10]\n",
      " [6.7610475e-12 1.3356514e-11 2.8563461e-11 ... 1.7589857e-11\n",
      "  7.8213182e-12 1.7808857e-11]\n",
      " [2.1527506e-10 3.2446153e-11 2.2457500e-10 ... 5.6930280e-11\n",
      "  2.4305721e-11 1.7760893e-10]\n",
      " ...\n",
      " [7.7550119e-12 2.8714909e-11 5.3211286e-11 ... 8.0331693e-11\n",
      "  4.5493062e-12 2.2141948e-11]\n",
      " [2.5368779e-10 2.1386629e-10 1.1641195e-09 ... 7.2804751e-10\n",
      "  5.5637241e-11 2.2818511e-10]\n",
      " [3.2676018e-11 8.5104328e-12 8.2302975e-12 ... 1.0251000e-11\n",
      "  5.3121140e-12 3.3897253e-11]]>, <Variable path=adam/sequential_dense_11_bias_momentum, shape=(100,), dtype=float32, value=[-1.1557331e-06  1.7772213e-06  3.2090190e-07  5.8738112e-07\n",
      " -1.4027929e-06 -2.3789653e-06  5.2445430e-06  1.4254448e-06\n",
      "  1.6483483e-06 -2.3636280e-06  2.6079933e-06  7.0636690e-07\n",
      " -2.2550114e-07 -3.0121573e-05 -3.3155493e-06 -1.1509555e-06\n",
      " -5.8977112e-06  6.9038657e-07  2.5725453e-06 -1.1576749e-06\n",
      "  1.2055998e-06 -1.8883943e-06 -5.3277208e-06 -2.3350203e-06\n",
      "  2.7845415e-06  4.4579615e-06 -8.8315228e-06  2.4481790e-06\n",
      " -6.8888348e-06 -4.8180286e-06 -1.7528712e-06  3.0203587e-06\n",
      " -2.0186453e-06  3.6511829e-06  3.1550559e-07 -3.3316280e-06\n",
      "  8.0443459e-07 -2.6646876e-07  2.2816478e-06 -2.1962601e-06\n",
      "  1.3079108e-06 -1.2453045e-06 -3.1562504e-06 -1.3211327e-06\n",
      "  5.9233075e-06  3.4464254e-06  2.2827969e-06  1.8024322e-05\n",
      "  4.7487197e-06  5.0770456e-07  1.2327621e-06 -4.0588299e-07\n",
      "  8.1299004e-07  5.0844110e-06 -5.5329760e-06 -6.9110088e-06\n",
      "  1.2500723e-06 -3.5046819e-06  1.0677777e-06 -1.5358129e-06\n",
      "  2.3987179e-06 -2.6137368e-06 -5.0348867e-06  2.9904127e-06\n",
      "  8.9481512e-07 -1.1306647e-06 -3.8219309e-06 -8.6487098e-06\n",
      "  5.4177735e-07 -2.1920721e-06  5.4597435e-06  4.9000359e-06\n",
      "  1.1706235e-05 -4.0887727e-07  2.9251005e-06 -8.1872822e-06\n",
      " -2.6270695e-06 -4.4733076e-07 -9.4332263e-06 -4.3906502e-06\n",
      " -1.9570363e-05 -4.0614887e-06  5.2155428e-06 -6.1979756e-07\n",
      " -1.1533167e-06 -2.3486693e-06 -7.7065415e-06 -2.1567625e-06\n",
      "  4.8385054e-06 -5.5110007e-07  1.8361357e-06  1.0732444e-06\n",
      "  1.2546138e-06  2.3716671e-06  6.9498151e-07  3.8938219e-06\n",
      "  5.5494002e-06 -1.9503386e-06 -1.7379374e-07  8.9711307e-07]>, <Variable path=adam/sequential_dense_11_bias_velocity, shape=(100,), dtype=float32, value=[2.78851431e-09 4.40923609e-09 5.84437432e-09 5.36905853e-09\n",
      " 1.15141063e-09 2.17571094e-09 1.09934879e-08 5.14816278e-09\n",
      " 2.68775846e-09 8.54899529e-09 1.27073978e-08 6.97367897e-09\n",
      " 1.03279230e-09 2.16819842e-07 3.43723294e-09 4.20370405e-10\n",
      " 5.78296877e-09 1.96639177e-10 7.66207098e-09 4.51766979e-09\n",
      " 3.17400399e-08 9.73747527e-10 7.48392193e-09 1.72756724e-08\n",
      " 4.71893324e-09 2.50793502e-08 2.57779327e-08 8.60317684e-09\n",
      " 1.50322172e-08 1.25805615e-08 1.48563464e-08 1.89294926e-08\n",
      " 6.62825439e-09 4.83112794e-09 9.23022492e-10 5.57148017e-09\n",
      " 8.65775540e-09 3.32196715e-10 1.66373693e-09 8.60256666e-09\n",
      " 1.44935042e-09 3.15921928e-10 4.55484761e-09 6.89053614e-09\n",
      " 1.49790473e-08 4.45347492e-09 4.47729098e-09 9.60322666e-08\n",
      " 1.29210971e-08 3.06568104e-10 4.87521401e-09 8.60643556e-10\n",
      " 5.27230659e-09 1.18300481e-08 1.70664531e-08 1.87768343e-08\n",
      " 6.71408262e-10 1.45787968e-08 1.14648335e-08 1.00603463e-08\n",
      " 6.94460001e-09 9.60236690e-09 8.69390249e-09 1.42398635e-08\n",
      " 5.93094995e-09 9.79625714e-10 1.07699929e-08 2.05355537e-08\n",
      " 1.02979509e-08 1.41485912e-09 1.18433290e-08 8.64975647e-09\n",
      " 5.02915256e-08 2.64911398e-10 1.86638065e-08 1.43337324e-08\n",
      " 9.58469126e-09 1.44168322e-09 2.61287063e-08 5.95659166e-09\n",
      " 1.46654742e-07 6.31232355e-09 6.74784761e-09 3.88809873e-10\n",
      " 5.46401979e-10 1.12945653e-09 1.90220018e-08 4.34983427e-09\n",
      " 8.34477021e-09 4.47210741e-10 4.48371207e-09 3.51120555e-08\n",
      " 1.26740363e-09 9.42177092e-09 9.23279142e-09 1.23792683e-08\n",
      " 2.38835369e-08 1.09808127e-08 9.38837563e-10 5.24365529e-09]>, <Variable path=adam/sequential_dense_12_kernel_momentum, shape=(100, 100), dtype=float32, value=[[-1.47761739e-07 -2.16484040e-07 -2.87584697e-07 ...  5.46677541e-07\n",
      "   1.21357395e-08 -2.39630936e-07]\n",
      " [ 8.24247024e-07 -5.18786507e-08  7.47595976e-08 ...  1.11375698e-06\n",
      "  -5.11526022e-09 -3.05591072e-07]\n",
      " [-1.16197953e-06  1.18545429e-07 -4.59017002e-08 ... -1.70590988e-06\n",
      "   1.50060480e-08  5.02275157e-07]\n",
      " ...\n",
      " [-8.83474797e-07 -7.78194647e-08 -1.12522997e-07 ... -1.15109731e-06\n",
      "   5.77393280e-08  2.21895817e-07]\n",
      " [-5.63330559e-07  1.06344601e-07  5.77896486e-08 ... -1.06135212e-06\n",
      "   2.77192491e-09  3.02190330e-07]\n",
      " [-5.81396534e-07 -2.89261415e-08 -1.51467191e-07 ... -2.52005435e-07\n",
      "  -3.50123912e-08  8.98802739e-08]]>, <Variable path=adam/sequential_dense_12_kernel_velocity, shape=(100, 100), dtype=float32, value=[[7.0951460e-11 2.9756642e-11 9.3179309e-10 ... 1.3782383e-09\n",
      "  5.9813296e-12 5.8113375e-11]\n",
      " [2.4960342e-10 1.1340674e-11 1.3695543e-10 ... 3.9213263e-10\n",
      "  4.0669744e-13 4.4514243e-11]\n",
      " [4.4919143e-10 3.8347089e-11 2.2942687e-10 ... 1.0336196e-09\n",
      "  8.4099784e-13 1.0449079e-10]\n",
      " ...\n",
      " [2.4380978e-10 3.8559468e-12 5.2536375e-10 ... 4.7302673e-10\n",
      "  8.0001873e-12 4.2544985e-11]\n",
      " [9.3672979e-11 1.6101473e-11 4.5352017e-11 ... 6.1594813e-10\n",
      "  2.7583526e-13 3.6270854e-11]\n",
      " [2.3302940e-10 7.2159021e-13 3.5713882e-10 ... 8.0152295e-11\n",
      "  1.8086102e-12 1.7323578e-11]]>, <Variable path=adam/sequential_dense_12_bias_momentum, shape=(100,), dtype=float32, value=[ 1.75905211e-06 -5.39491566e-07 -4.28396874e-07 -1.32653690e-06\n",
      "  2.35226054e-07  4.09563427e-07  4.16595526e-07 -2.51517554e-06\n",
      " -2.05635104e-07 -3.52637130e-06  5.55417543e-08 -6.04195884e-06\n",
      "  2.54317001e-07 -5.29320232e-06 -1.16798219e-06 -3.82802284e-07\n",
      " -1.23353914e-07 -1.34865013e-06 -9.17718808e-08  2.12032069e-06\n",
      " -9.52850087e-07  9.53369522e-07 -5.28979672e-06  2.89839647e-07\n",
      "  1.90079746e-07 -3.79962103e-06 -9.44826979e-07  2.17744446e-06\n",
      "  2.17244155e-06 -1.44436818e-08 -1.64734547e-06  6.39106815e-07\n",
      " -2.63735592e-06  2.37285371e-06 -1.16791853e-06 -3.21728777e-07\n",
      "  1.06350001e-07 -1.06091454e-06  1.07882556e-06  6.66308779e-07\n",
      "  3.17381546e-06  4.28166550e-06  5.76780849e-07 -4.08620934e-07\n",
      "  1.59586094e-07  2.39368137e-06 -8.03346779e-07  6.53822525e-08\n",
      "  2.54234055e-06 -1.85841486e-07  1.08675147e-06 -1.73677535e-07\n",
      "  1.85974585e-07 -2.20052982e-07  9.03823206e-07  1.71539773e-06\n",
      "  4.14954457e-06  1.58931670e-07  2.90411162e-06 -2.47189405e-07\n",
      "  1.52139364e-06  1.72044054e-07 -1.04295145e-06  1.29235036e-06\n",
      "  5.37608514e-07  1.21334445e-08  1.04415039e-06 -7.26441954e-07\n",
      " -1.39673557e-06 -1.28531624e-07  5.05899470e-06 -1.30204319e-06\n",
      " -6.47218144e-07 -6.01262627e-07  2.86327031e-06 -1.04869457e-06\n",
      " -1.47560295e-05  1.22043502e-06  1.73264402e-06 -4.72866546e-08\n",
      "  1.05125821e-06 -2.87695173e-07 -9.50686285e-07 -2.58161663e-06\n",
      " -2.34406048e-06  1.03115482e-07  1.47790024e-06 -4.06814650e-07\n",
      " -5.85330895e-09 -4.05105584e-06 -6.63348601e-07  1.07758558e-06\n",
      " -2.93968810e-07 -3.09834689e-07 -3.75740512e-07  7.19745046e-08\n",
      "  1.40839029e-06  4.05551145e-06  4.92290333e-08 -1.11875488e-06]>, <Variable path=adam/sequential_dense_12_bias_velocity, shape=(100,), dtype=float32, value=[8.8109325e-10 3.2527961e-10 2.7075466e-09 9.3090557e-10 4.7820452e-11\n",
      " 3.7785633e-10 3.9035580e-10 5.2590994e-09 1.6467597e-10 5.5636096e-09\n",
      " 3.6498216e-10 1.3171223e-08 6.9216299e-11 1.7565792e-08 2.0899953e-09\n",
      " 2.9794506e-10 2.7185037e-11 6.0555039e-10 1.2924631e-09 5.0449276e-09\n",
      " 5.0085718e-09 6.5853867e-10 1.3993009e-08 3.8984313e-10 2.6998888e-11\n",
      " 1.2518014e-08 4.1958215e-09 2.8570626e-09 1.4408779e-09 1.0673327e-11\n",
      " 9.7982011e-10 4.7892592e-09 2.5646351e-08 5.9874261e-09 2.8911564e-09\n",
      " 2.8236817e-11 1.0145430e-11 5.3398513e-10 1.7094792e-09 4.6636855e-10\n",
      " 3.5146834e-09 1.0444444e-08 3.7161785e-10 7.0637024e-11 9.9849400e-12\n",
      " 2.6479920e-09 9.7426400e-10 4.3033607e-10 7.2386013e-09 4.6690735e-11\n",
      " 1.0691098e-09 4.9676731e-11 4.4439934e-11 1.3858345e-10 4.4653048e-10\n",
      " 2.7316274e-09 1.8811681e-08 1.1300153e-09 1.2278876e-08 2.3147292e-10\n",
      " 8.5008767e-10 8.7775325e-12 3.4728048e-10 2.9851734e-09 5.7755123e-10\n",
      " 2.2814663e-10 8.1626617e-10 2.1422080e-09 7.8406592e-10 6.9547346e-10\n",
      " 1.7524709e-08 6.3258160e-10 1.0842340e-09 1.1868094e-10 7.6195139e-09\n",
      " 1.3343849e-09 4.3359922e-08 2.6819758e-09 5.0394369e-09 6.3135651e-12\n",
      " 5.0813187e-10 2.8786581e-10 2.0987428e-10 2.2048847e-09 8.6622265e-09\n",
      " 8.7537260e-12 1.2317670e-09 3.0079864e-10 7.2735401e-11 1.8902739e-08\n",
      " 7.5941708e-10 1.6506938e-09 1.5416443e-09 1.3130241e-10 8.6935813e-11\n",
      " 2.8980945e-12 2.1615929e-09 1.6456653e-08 2.5029055e-11 5.8670696e-10]>, <Variable path=adam/sequential_dense_13_kernel_momentum, shape=(100, 100), dtype=float32, value=[[ 2.5967043e-07 -5.8480982e-06 -3.5263201e-06 ...  4.6391750e-07\n",
      "   6.0314153e-07  9.6776972e-08]\n",
      " [ 4.0876864e-07  8.6268701e-06  3.0202259e-06 ... -5.6111287e-07\n",
      "  -2.6391746e-07  1.3777881e-07]\n",
      " [ 4.3845051e-07  5.8773321e-06 -2.2540814e-07 ...  1.3481730e-07\n",
      "   1.6331808e-07  1.0587158e-07]\n",
      " ...\n",
      " [-1.7658971e-07 -1.4039806e-06  3.6266289e-07 ... -1.6695608e-07\n",
      "   7.1459816e-08 -3.0125555e-08]\n",
      " [ 6.3012794e-07  8.6702148e-06  1.2322539e-06 ...  1.7875006e-07\n",
      "  -7.2111995e-08  1.5154889e-07]\n",
      " [ 5.1465526e-07  9.0278190e-06  5.3177787e-06 ... -9.3141068e-07\n",
      "  -6.0172533e-07  1.4684667e-07]]>, <Variable path=adam/sequential_dense_13_kernel_velocity, shape=(100, 100), dtype=float32, value=[[1.3487904e-10 1.2822408e-08 1.8640200e-08 ... 1.6721330e-10\n",
      "  2.8056696e-10 8.3881313e-12]\n",
      " [1.5268335e-10 2.6536657e-08 9.9143422e-09 ... 3.4105385e-10\n",
      "  9.2663160e-10 9.0181274e-12]\n",
      " [1.1366885e-10 1.1590021e-08 1.4066039e-09 ... 6.8863672e-11\n",
      "  2.3223055e-10 3.8964053e-12]\n",
      " ...\n",
      " [5.5596298e-11 7.0614581e-10 7.2399620e-10 ... 1.7797364e-11\n",
      "  1.3672163e-10 1.9627308e-12]\n",
      " [3.6352993e-10 2.6638210e-08 1.2337911e-09 ... 4.6321211e-11\n",
      "  7.0847567e-10 7.1496142e-12]\n",
      " [7.9515250e-10 2.9767859e-08 3.2695521e-08 ... 7.4893880e-10\n",
      "  1.6086388e-09 1.0825843e-11]]>, <Variable path=adam/sequential_dense_13_bias_momentum, shape=(100,), dtype=float32, value=[-5.66143399e-07 -8.04819410e-06  1.21204675e-06  9.15123394e-07\n",
      " -1.34998750e-08 -4.37705694e-07 -6.02595719e-07 -9.11215125e-07\n",
      " -1.64749940e-06  3.80648748e-06 -1.62277107e-08  3.95452730e-07\n",
      " -1.75081175e-06  9.49737114e-07 -1.27635325e-09 -2.89539503e-09\n",
      " -1.78619608e-09  2.22002200e-07  5.79955952e-07  1.41710689e-06\n",
      " -5.34797755e-06 -3.59641490e-07  9.18831574e-07  5.18719207e-06\n",
      " -4.04621369e-06  7.20235812e-06 -1.43692932e-06 -3.58401564e-07\n",
      " -1.01970127e-05 -1.98261478e-06 -8.35163519e-07 -1.33711876e-06\n",
      "  7.53799532e-06  3.01514547e-09  1.67312294e-06 -3.99433929e-07\n",
      " -2.20619950e-06 -6.62337641e-07 -2.34874574e-06 -4.24238124e-06\n",
      " -8.74803504e-07 -1.07538849e-08 -4.29858233e-07  8.79389518e-07\n",
      " -1.06095865e-06 -1.27522014e-07 -5.37921005e-06  2.41928910e-06\n",
      " -4.23979736e-06  4.88809029e-08  2.73285139e-07  1.65687925e-06\n",
      " -1.38684683e-07  1.58934326e-05 -1.79406993e-06  7.16014370e-07\n",
      " -8.12149352e-08  1.41815789e-07 -1.82804638e-06 -5.97153473e-08\n",
      " -1.95597337e-07  1.22822019e-08 -8.50596393e-09  5.03488362e-09\n",
      "  2.34892650e-06 -8.43512851e-08 -8.02828652e-07 -2.08558504e-10\n",
      "  1.62953972e-06  1.25548593e-06 -5.01211252e-06  6.02968021e-06\n",
      "  1.07342394e-05  3.08293784e-06  3.00921602e-06 -3.11838426e-06\n",
      "  2.96734743e-06 -4.37043354e-07  4.74437456e-07  8.00792236e-07\n",
      "  3.08702482e-07 -4.35885977e-06  1.60142449e-07 -4.40336862e-06\n",
      "  5.58447482e-07 -1.57174753e-07 -1.82793235e-06  1.02640556e-07\n",
      "  1.28425654e-06  2.57762076e-06  2.79373211e-07 -1.36563699e-07\n",
      " -2.96793996e-06  8.99282895e-07  2.15804471e-06  3.49910192e-06\n",
      " -3.59498927e-07 -1.82570830e-07 -4.06412596e-07 -1.61513540e-07]>, <Variable path=adam/sequential_dense_13_bias_velocity, shape=(100,), dtype=float32, value=[2.14246260e-10 2.02308463e-08 9.25123977e-09 1.81807214e-09\n",
      " 5.89426918e-14 4.59623145e-10 4.96727270e-10 1.10950431e-08\n",
      " 1.48475288e-09 1.61503539e-08 9.57914778e-14 1.65164452e-10\n",
      " 1.73516024e-09 5.33072919e-10 4.38146589e-13 1.43591438e-13\n",
      " 2.68987544e-14 1.53795379e-10 5.65411329e-10 1.81201942e-09\n",
      " 1.44137422e-08 1.04897124e-09 3.78661325e-09 8.40679260e-09\n",
      " 1.95790673e-08 1.58077071e-08 1.55044044e-09 1.14204224e-09\n",
      " 4.20879473e-08 2.74062351e-09 2.14091433e-09 8.86762219e-10\n",
      " 4.20542676e-08 3.79019028e-13 3.51366025e-09 1.41579512e-10\n",
      " 2.01728034e-09 2.75231338e-10 1.10695622e-08 3.38543344e-08\n",
      " 1.63814673e-09 2.02844130e-14 1.63326386e-09 2.33547115e-09\n",
      " 1.17097425e-08 6.11109774e-10 9.45981959e-09 6.39763531e-09\n",
      " 6.99558367e-09 3.51961689e-11 1.26250302e-10 2.46353249e-08\n",
      " 4.59834601e-11 7.60429089e-08 3.42016122e-08 6.35204689e-09\n",
      " 9.82112482e-12 3.39073924e-09 4.28487423e-09 5.46085221e-09\n",
      " 1.04008475e-10 3.07336649e-13 2.08894371e-13 9.42443755e-14\n",
      " 3.47686413e-09 2.04107616e-11 1.63532565e-09 1.81747480e-13\n",
      " 1.81347881e-09 1.70834247e-09 2.11274070e-07 3.22387308e-08\n",
      " 3.73210121e-08 1.12421530e-08 8.14458634e-09 5.03251796e-09\n",
      " 1.05661622e-08 3.04627906e-10 6.69754807e-11 6.97660985e-10\n",
      " 2.34900011e-09 1.36299917e-07 1.08590151e-11 1.75895440e-08\n",
      " 3.22577653e-09 1.46798320e-11 7.59440599e-09 1.62560243e-10\n",
      " 3.13788373e-09 1.37999496e-08 5.55181665e-11 9.82991910e-09\n",
      " 4.93641261e-09 2.64387706e-10 5.27715160e-09 1.66795946e-08\n",
      " 1.53273963e-10 1.91608951e-10 6.73574585e-10 1.57056763e-11]>, <Variable path=adam/sequential_dense_14_kernel_momentum, shape=(100, 2), dtype=float32, value=[[-1.92416192e-04  6.84589613e-05]\n",
      " [ 6.09509589e-05 -4.84546617e-05]\n",
      " [-1.68541359e-04  5.40585606e-05]\n",
      " [-1.85754790e-04  6.76219206e-05]\n",
      " [ 2.88209321e-07 -4.30477121e-06]\n",
      " [-1.81414551e-04  6.16697507e-05]\n",
      " [ 1.91698229e-04 -6.80894518e-05]\n",
      " [ 1.55178990e-04 -6.56911288e-05]\n",
      " [ 1.33766313e-04 -4.09373788e-05]\n",
      " [ 1.70393207e-04 -6.45987166e-05]\n",
      " [ 2.18725700e-06 -1.03098000e-06]\n",
      " [ 1.91389991e-04 -6.82807586e-05]\n",
      " [ 1.59008137e-04 -5.57890089e-05]\n",
      " [ 1.67037273e-04 -5.40565488e-05]\n",
      " [ 2.65598328e-06 -9.86456371e-07]\n",
      " [-1.28963211e-06  1.72013870e-08]\n",
      " [ 9.09549271e-06  1.45639615e-06]\n",
      " [-1.93321408e-04  6.81039892e-05]\n",
      " [-1.90810213e-04  6.85219929e-05]\n",
      " [ 1.62924116e-04 -6.87711072e-05]\n",
      " [ 1.03915212e-04 -5.52449419e-05]\n",
      " [-1.79245399e-04  6.05464520e-05]\n",
      " [ 1.59233838e-04 -4.93338339e-05]\n",
      " [-6.83444669e-05  5.34534483e-05]\n",
      " [ 1.55009373e-04 -6.11159776e-05]\n",
      " [ 4.02944424e-05 -4.25298058e-05]\n",
      " [ 1.83008029e-04 -6.30068243e-05]\n",
      " [-1.90220977e-04  6.70363661e-05]\n",
      " [ 3.44078799e-05 -4.47262319e-05]\n",
      " [-1.64962723e-04  5.38668282e-05]\n",
      " [-1.85366836e-04  6.70709705e-05]\n",
      " [ 1.89909828e-04 -6.88920773e-05]\n",
      " [-1.33128138e-04  5.64636211e-05]\n",
      " [-3.54300664e-06  1.37516838e-06]\n",
      " [ 1.57175353e-04 -5.33257298e-05]\n",
      " [-1.89588725e-04  6.77190837e-05]\n",
      " [-1.10463356e-04  5.65484806e-05]\n",
      " [-1.90348161e-04  6.63679675e-05]\n",
      " [-1.75496680e-04  6.54687756e-05]\n",
      " [ 1.45248123e-04 -4.04977181e-05]\n",
      " [ 1.83189637e-04 -6.24053573e-05]\n",
      " [-1.94955224e-04  6.88123691e-05]\n",
      " [ 1.35172028e-04 -5.61656561e-05]\n",
      " [ 1.56875773e-04 -5.46813026e-05]\n",
      " [ 1.52234046e-04 -4.58253853e-05]\n",
      " [ 1.91439729e-04 -6.80359590e-05]\n",
      " [-1.03400816e-04  3.92982911e-05]\n",
      " [-1.65138583e-04  6.01301217e-05]\n",
      " [-4.59645744e-05 -2.31518879e-05]\n",
      " [ 1.93578424e-04 -6.83970575e-05]\n",
      " [-1.92572130e-04  6.82702084e-05]\n",
      " [ 1.52597728e-04 -6.68523498e-05]\n",
      " [-1.93932938e-04  6.88098880e-05]\n",
      " [-6.73596287e-06  3.67856846e-05]\n",
      " [-1.18287062e-04  2.75954881e-05]\n",
      " [-1.55937800e-04  6.47857087e-05]\n",
      " [ 1.94615393e-04 -6.87977445e-05]\n",
      " [ 1.48090723e-04 -5.99887826e-05]\n",
      " [ 1.63119650e-04 -5.43716451e-05]\n",
      " [-8.68196366e-05  5.36086591e-05]\n",
      " [ 1.92346313e-04 -6.81749807e-05]\n",
      " [ 4.64431622e-07 -3.44243574e-07]\n",
      " [-1.22944937e-06 -2.60875413e-06]\n",
      " [-4.57039778e-06  6.12540418e-09]\n",
      " [-1.63289980e-04  5.34870160e-05]\n",
      " [-2.95767713e-05 -6.88971886e-06]\n",
      " [ 1.80514413e-04 -6.16317848e-05]\n",
      " [-4.37721428e-06 -4.45220167e-07]\n",
      " [-1.87247788e-04  6.92107424e-05]\n",
      " [ 1.80209696e-04 -6.17203550e-05]\n",
      " [-1.33590074e-04  2.12334489e-05]\n",
      " [-1.39563490e-04  6.70255540e-05]\n",
      " [-1.19831284e-05  3.87493856e-05]\n",
      " [ 1.69490522e-04 -5.76472521e-05]\n",
      " [ 1.63295117e-04 -5.19191817e-05]\n",
      " [-7.72572675e-05  4.94684573e-05]\n",
      " [-1.85756944e-04  6.32352021e-05]\n",
      " [ 1.92074280e-04 -6.83092949e-05]\n",
      " [-1.34124930e-04  6.83083636e-05]\n",
      " [ 1.81548530e-04 -6.19894126e-05]\n",
      " [ 1.75261186e-04 -7.00837263e-05]\n",
      " [ 1.10104622e-04 -1.72583750e-05]\n",
      " [ 1.94217268e-04 -6.89245571e-05]\n",
      " [ 8.97131686e-05 -5.28834425e-05]\n",
      " [-1.73749402e-04  6.86153653e-05]\n",
      " [-1.94332795e-04  6.88773871e-05]\n",
      " [-1.41418364e-04  6.12292279e-05]\n",
      " [-1.89009166e-04  6.66556298e-05]\n",
      " [ 4.73478794e-05 -4.51096566e-05]\n",
      " [-1.05115381e-04  5.66352464e-05]\n",
      " [ 1.86208767e-04 -6.31931689e-05]\n",
      " [ 1.46802864e-04 -5.87419636e-05]\n",
      " [-1.05048748e-04  5.72950848e-05]\n",
      " [-1.45128579e-04  7.52079359e-05]\n",
      " [-1.47374187e-04  4.21636942e-05]\n",
      " [ 1.69404113e-04 -6.52598319e-05]\n",
      " [ 1.92786843e-04 -6.84140687e-05]\n",
      " [ 1.89151528e-04 -6.71463786e-05]\n",
      " [-1.88116377e-04  6.81045931e-05]\n",
      " [-1.94355642e-04  6.88302098e-05]]>, <Variable path=adam/sequential_dense_14_kernel_velocity, shape=(100, 2), dtype=float32, value=[[3.29439899e-05 1.39071353e-05]\n",
      " [8.13558836e-06 1.59174692e-06]\n",
      " [2.33333303e-05 1.03533039e-05]\n",
      " [3.06966285e-05 1.29970886e-05]\n",
      " [2.10585416e-08 3.00806047e-09]\n",
      " [2.88327046e-05 1.11458330e-05]\n",
      " [3.26341906e-05 1.37708821e-05]\n",
      " [2.35458301e-05 7.44425870e-06]\n",
      " [1.47783321e-05 5.67112238e-06]\n",
      " [2.71642675e-05 9.73299757e-06]\n",
      " [8.59675553e-10 3.79657361e-10]\n",
      " [3.26212685e-05 1.37590969e-05]\n",
      " [2.31985705e-05 8.02829709e-06]\n",
      " [2.42512306e-05 8.61240369e-06]\n",
      " [7.50179086e-10 1.88096858e-10]\n",
      " [2.29420238e-09 6.06531991e-10]\n",
      " [3.92775554e-08 2.01574757e-09]\n",
      " [3.30185794e-05 1.40017255e-05]\n",
      " [3.26160734e-05 1.36730469e-05]\n",
      " [2.56821604e-05 8.74569287e-06]\n",
      " [1.33254707e-05 3.26127429e-06]\n",
      " [2.79940850e-05 1.11189211e-05]\n",
      " [2.12258255e-05 7.38094604e-06]\n",
      " [9.94798484e-06 2.40139980e-06]\n",
      " [2.25754629e-05 7.08242078e-06]\n",
      " [6.53607640e-06 1.19757294e-06]\n",
      " [2.96343715e-05 1.16289702e-05]\n",
      " [3.19098217e-05 1.35381042e-05]\n",
      " [6.42127225e-06 1.29397893e-06]\n",
      " [2.37988115e-05 8.37084099e-06]\n",
      " [3.07490373e-05 1.27613439e-05]\n",
      " [3.26293994e-05 1.36113740e-05]\n",
      " [1.76515805e-05 4.73148702e-06]\n",
      " [4.22951230e-09 9.94913596e-10]\n",
      " [2.21657901e-05 7.59104796e-06]\n",
      " [3.20446452e-05 1.33308577e-05]\n",
      " [1.31331153e-05 2.68289432e-06]\n",
      " [3.17056474e-05 1.36111530e-05]\n",
      " [2.80064123e-05 1.10702231e-05]\n",
      " [1.61087319e-05 5.96437121e-06]\n",
      " [2.94928741e-05 1.15540106e-05]\n",
      " [3.35674049e-05 1.42792542e-05]\n",
      " [1.83997345e-05 5.30971192e-06]\n",
      " [2.23183943e-05 7.52381402e-06]\n",
      " [1.87417554e-05 6.56941347e-06]\n",
      " [3.24984503e-05 1.37569141e-05]\n",
      " [1.15721332e-05 3.90167725e-06]\n",
      " [2.49841323e-05 8.75206115e-06]\n",
      " [1.98871999e-06 1.45918733e-07]\n",
      " [3.31578485e-05 1.40932725e-05]\n",
      " [3.28989227e-05 1.38893502e-05]\n",
      " [2.27479450e-05 6.99510474e-06]\n",
      " [3.34001816e-05 1.41602241e-05]\n",
      " [4.28001613e-06 6.70052543e-07]\n",
      " [7.92452283e-06 3.31928641e-06]\n",
      " [2.37143195e-05 7.54956591e-06]\n",
      " [3.35127916e-05 1.42392973e-05]\n",
      " [2.08272249e-05 7.15039914e-06]\n",
      " [2.35558255e-05 8.35620176e-06]\n",
      " [1.11699092e-05 2.40575537e-06]\n",
      " [3.28089372e-05 1.39118411e-05]\n",
      " [6.70820399e-10 1.65336536e-10]\n",
      " [1.37070977e-09 1.10953424e-09]\n",
      " [4.85427520e-09 5.22631771e-10]\n",
      " [2.34195704e-05 8.21114554e-06]\n",
      " [3.13835471e-07 4.77267044e-08]\n",
      " [2.86239265e-05 1.10554111e-05]\n",
      " [4.79700413e-09 7.45956741e-10]\n",
      " [3.21603875e-05 1.32930973e-05]\n",
      " [2.86756494e-05 1.08951044e-05]\n",
      " [8.53183337e-06 3.11409894e-06]\n",
      " [2.02025094e-05 5.70080556e-06]\n",
      " [4.67516566e-06 7.61534125e-07]\n",
      " [2.60138549e-05 9.09448136e-06]\n",
      " [2.25988351e-05 8.16476950e-06]\n",
      " [9.91787419e-06 2.15128466e-06]\n",
      " [2.96552589e-05 1.30100325e-05]\n",
      " [3.27553207e-05 1.38817659e-05]\n",
      " [1.86140460e-05 4.96510665e-06]\n",
      " [2.89828404e-05 1.11240979e-05]\n",
      " [2.90285461e-05 1.10852452e-05]\n",
      " [4.89382182e-06 2.41063776e-06]\n",
      " [3.35082659e-05 1.42282197e-05]\n",
      " [1.12733969e-05 2.50666199e-06]\n",
      " [2.78538828e-05 1.06712678e-05]\n",
      " [3.35087752e-05 1.42340214e-05]\n",
      " [2.02931515e-05 5.94007270e-06]\n",
      " [3.15630969e-05 1.32241976e-05]\n",
      " [7.21034849e-06 1.34733750e-06]\n",
      " [1.36183799e-05 3.21059088e-06]\n",
      " [3.04486130e-05 1.20900158e-05]\n",
      " [2.11417446e-05 6.58285671e-06]\n",
      " [1.36828512e-05 3.14219574e-06]\n",
      " [2.17382149e-05 6.88458886e-06]\n",
      " [1.68885781e-05 6.18464583e-06]\n",
      " [2.67891191e-05 9.79822380e-06]\n",
      " [3.30052026e-05 1.39666690e-05]\n",
      " [3.16681690e-05 1.33439471e-05]\n",
      " [3.15511606e-05 1.33684889e-05]\n",
      " [3.34891229e-05 1.42376239e-05]]>, <Variable path=adam/sequential_dense_14_bias_momentum, shape=(2,), dtype=float32, value=[-8.1678867e-05 -1.3841953e-05]>, <Variable path=adam/sequential_dense_14_bias_velocity, shape=(2,), dtype=float32, value=[2.0980010e-06 3.3832208e-07]>]\n"
     ]
    }
   ],
   "source": [
    "#np.savetxt('/kaggle/working/optimizer.txt', optim.variables)\n",
    "print(optim.variables)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f024de0",
   "metadata": {
    "papermill": {
     "duration": 0.080842,
     "end_time": "2025-02-06T17:07:19.390864",
     "exception": false,
     "start_time": "2025-02-06T17:07:19.310022",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Loss"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6413701,
     "sourceId": 10356489,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6583681,
     "sourceId": 10633705,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6614454,
     "sourceId": 10677865,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 214057,
     "modelInstanceId": 192111,
     "sourceId": 225222,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 216645,
     "modelInstanceId": 194744,
     "sourceId": 228383,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 232897,
     "modelInstanceId": 211206,
     "sourceId": 247133,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 236677,
     "modelInstanceId": 214985,
     "sourceId": 251490,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 23257.245801,
   "end_time": "2025-02-06T17:07:23.114411",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-06T10:39:45.868610",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
